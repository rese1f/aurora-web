<!doctype html>
<html lang="en">
    <head>
        <title>AuroraCap: A Detailed Captioning Baseline and Benchmark for Video</title>
        <link rel="icon" type="image/x-icon" href="/static/img/icons/web_icon.ico">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://aurora-web.github.io/" />
        <meta property="og:image" content="https://aurora-web.github.io/static/img/preview.png" />
        <meta property="og:title" content="AuroraCap: A Detailed Captioning Baseline and Benchmark for Video" />
        <meta property="og:description" content="AuroraCap is a multimodal LLM designed for image and video detailed captioning. We also release VDC, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets." />
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://aurora-web.github.io/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://aurora-web.github.io/static/img/preview.png" />
        <meta name="twitter:title" content="AuroraCap: A Detailed Captioning Baseline and Benchmark for Video" />
        <meta name="twitter:description" content="AuroraCap is a multimodal LLM designed for image and video detailed captioning. We also release VDC, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets." />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>

        <style>
            .small-caps {
                font-variant: small-caps;
            }
            .pdf-container {
            width: 80%; 
            max-width: 1200px; 
            height: 600px;
            margin: 0 auto; 
            border: 1px solid #ccc; 
        }
        embed {
            width: 100%;
            height: 100%;
        }
        .highlight-red {
            background-color: #b83a4b36;
            font-weight: bold;
        }

        </style>

    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px">AuroraCap</h1>
                    <h2>Less is More for Efficient Video Detailed Captioning and a New Benchmark</h2>
                        <p>
                            AuroraCap is a multimodal LLM designed for image and video detailed captioning. We also release VDC, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets.
                        </p>
                    

                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>PDF</span>
                        </a>
                        <!-- replace image -->
                        <a href="https://github.com/rese1f/aurora" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>GitHub</span>
                        </a>
                        <!-- <br> -->
                        <a href="https://huggingface.co/collections/Reself/auroracap-66d117ffe13bedda96702013" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Model</span>
                        </a>
                        <a href="https://huggingface.co/datasets/Reself/AuroraCap-trainset" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Data</span>
                        </a>                        
                        <a href="https://huggingface.co/datasets/Reself/Video-Detailed-Caption" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>VDC Benchmark</span>
                        </a>
                    </div>
                </div>
                <div class="header-content">
                    <div class="icon-item">
                        <img src="./static/img/icons/obs.svg" alt="Observation Icon">
                        <div><strong>Observation</strong>: We found that image-based LLaVA-like multimodal large language models can be easily adapted to a video one without any additional parameters but only with high-quality video-text instruction data for finetuning.</div>
                    </div>
                    <br>
                    <div class="icon-item">
                        <img src="./static/img/icons/eff.svg" alt="Efficency Icon">
                        <div><strong>Efficency</strong>: We can reduce the number of token used for image or video before injecting into LLM with marginal performance drop. Therefore, we propose AuroraCap, which is the state-of-the-art open-sourced video captioning model.</div>
                    </div>
                    <br>
                    <div class="icon-item">
                        <img src="./static/img/icons/benchmark.svg" alt="Benchmark Icon">
                        <div><strong>Benchmark</strong>: We also release VDC, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets.</div>
                    </div>
                </div>
            </div>
        </div>
        
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://rese1f.github.io/" class="author-link" target="_blank">Wenhao Chai</a><sup> 1,2</sup> &emsp;
                    <a href="https://espere-1119-song.github.io/" class="author-link" target="_blank">Enxin Song</a> &emsp;
                    <a href="https://yilundu.github.io/" class="author-link" target="_blank">Yilun Du</a><sup> 4,5</sup> &emsp;
                    <a href="https://cs.stanford.edu/~chenlin/" class="author-link" target="_blank">Chenlin Meng</a><sup> 2,3</sup> &emsp;
                    <a href="https://scholar.google.com/citations?user=WjF1dugAAAAJ" class="author-link" target="_blank">Vashisht Madhavan</a><sup> 2</sup> &emsp;
                    <a href="https://omerbt.github.io/" class="author-link" target="_blank">Omer Bar-Tal</a><sup> 2</sup> &emsp;
                    <a href="https://people.ece.uw.edu/hwang/" class="author-link" target="_blank">Jeng-Neng Hwang</a><sup> 1</sup> &emsp;
                    <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie</a><sup> 6</sup> &emsp;
                    <a href="https://nlp.stanford.edu/~manning/" class="author-link" target="_blank">Christopher D. Manning</a><sup> 3</sup> &emsp;
                    </p>
                <p>
                    <sup>1</sup> <a href="https://www.washington.edu/" class="affiliation-link" id="affiliation" target="_blank">University of Washington</a> &emsp;
                    <sup>2</sup> <a href="https://pika.art" class="affiliation-link" id="affiliation" target="_blank">Pika Lab</a> &emsp;
                    <sup>3</sup> <a href="https://www.stanford.edu/" class="affiliation-link" id="affiliation" target="_blank">Stanford University</a> &emsp;
                    <br>
                    <sup>4</sup> <a href="https://www.mit.edu/" class="affiliation-link" id="affiliation" target="_blank">Massachusetts Institute of Technology</a> &emsp;
                    <sup>5</sup> <a href="https://www.harvard.edu/" class="affiliation-link" id="affiliation" target="_blank">Harvard University</a> &emsp;
                    <br>
                    <sup>6</sup> <a href="https://www.nyu.edu/" class="affiliation-link" id="affiliation" target="_blank">New York University</a>
                </p>
            </div>
        </div>
        <!-- <p style="text-align: center;">
            <span class="author-note"><sup>*</sup>Project lead</span>&emsp;
            <span class="author-note"><sup>&dagger;</sup>Corresponding author</span>
        </p> -->
        

        
        <p class="text abstract">
            AuroraCap is a efficientmultimodal LLM designed for image and video detailed captioning. 
            We also release <span class="small-caps">VDC</span>, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets.
            <br>
            <ol class="text">
                <li style="list-style-type: none;"><strong><a href="#training-recipe">&sect; Model: AuroraCap</a></strong>: We illustrate how we can reduce the number of token used for image or video before injecting into LLM with marginal performance drop.</li>
                <li style="list-style-type: none;"><strong><a href="#vdc">&sect; Benchmark: Video Detailed Caption (VDC)</a></strong>: We present <span class="small-caps">VDC</span>, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets.</li>
            </ol>
        </p>

        <!-- <div class="icon-row">
            <a href="#training-recipe" class="icon-link">
                <img src="static/img/icons/visual.svg" alt="Visual Representation Logo" class="icon">
                Model: AuroraCap
            </a>
            <a href="#vdc" class="icon-link">
                <img src="static/img/icons/connector.svg" alt="Connector Logo" class="icon">
                Benchmark: VDC
            </a>
        </div> -->

        <!-- <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p> -->


        <p class="text abstract">
            We provide <a href="https://huggingface.co/collections/Reself/auroracap-66d117ffe13bedda96702013" target="_blank">model weights</a>, 
            <a href="https://github.com/rese1f/aurora" target="_blank">code</a>, 
            <a href="https://huggingface.co/datasets/Reself/AuroraCap-trainset" target="_blank">datasets</a>, 
            and <a href="https://huggingface.co/datasets/Reself/Video-Detailed-Caption" target="_blank">benchmark</a>. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.
        </p>

        

        <hr>

        <div id='training-recipe' class="vision-block">
            <div class="sub-section">
                <h1 class="text">AuroraCap: A Efficient Video Detailed Captioner</h1>

                    <p class="text">
                        <p class="text">
                        <h2 class="text">Architecture</h2>
                        <p class="text">
                            <strong>LLaVA.</strong> 
                            To effectively leverage the capabilities of both the pre-trained LLM and visual model, LLaVA adapt a simple multilayer perceptron (MLP) projection layer to connect each patch tokens of image features into the word embedding space. 
                            The original LLaVA model is trained by a two-stage instruction-tuning procedure, which first pretraining projection layer for feature alignment and then finetuning end-to-end while freeze the visual encoder. 
                            We adapt some conclusion from recent works for training the model.
                            We use the the training loss among the last ten iterations in original LLaVA alignment pretraining stage to guidance the ViT and LLM backbones selection.
                            <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                                <div class="table-container">
                                    <table class="data-table">
                                        <thead>
                                        <tr>
                                            <th colspan="1" class="tb-hdr">ViT</th>
                                            <th colspan="1" class="tb-hdr">ViT Size</th>
                                            <th colspan="1" class="tb-hdr">LLM</th>
                                            <th colspan="1" class="tb-hdr">LLM Size</th>
                                            <th colspan="1" class="tb-hdr">Loss</th>
                                        </tr>
                                        </thead>
                                        <tbody>
                                        <tr>
                                            <td class="section-border">facebook/dinov2-giant</td>
                                            <td>1,136M</td>
                                            <td class="section-border">microsoft/phi-2</td>
                                            <td>2.7B</td>
                                            <td>3.3021</td>
                                        </tr>
                                        <tr>
                                            <td class="section-border">openai/clip-vit-large-patch14-336</td>
                                            <td>428M</td>
                                            <td class="section-border">Qwen/Qwen1.5-0.5B-Chat</td>
                                            <td>0.5B</td>
                                            <td>3.1001</td>
                                        </tr>
                                        <tr>
                                            <td class="section-border">openai/clip-vit-large-patch14-336</td>
                                            <td>428M</td>
                                            <td class="section-border">microsoft/phi-2</td>
                                            <td>2.7B</td>
                                            <td>2.8067</td>
                                        </tr>
                                        <tr>
                                            <td class="section-border">laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</td>
                                            <td>1,845M</td>
                                            <td class="section-border">microsoft/phi-2</td>
                                            <td>2.7B</td>
                                            <td>2.7124</td>
                                        </tr>
                                        <tr>
                                            <td class="section-border">facebook/dinov2-giant</td>
                                            <td>1,136M</td>
                                            <td class="section-border">lmsys/vicuna-13b-v1.5</td>
                                            <td>13B</td>
                                            <td>2.3895</td>
                                        </tr>
                                        <tr>
                                            <td class="section-border">laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</td>
                                            <td>1,845M</td>
                                            <td class="section-border">internlm/internlm2-chat-7b</td>
                                            <td>7B</td>
                                            <td>2.3437</td>
                                        </tr>
                                        <tr>
                                            <td class="section-border">laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</td>
                                            <td>1,845M</td>
                                            <td class="section-border">internlm/internlm2-chat-20b</td>
                                            <td>20B</td>
                                            <td>2.2745</td>
                                        </tr>
                                        <tr>
                                            <td class="section-border">laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</td>
                                            <td>1,845M</td>
                                            <td class="section-border">deepseek-ai/deepseek-llm-67b-chat</td>
                                            <td>67B</td>
                                            <td>2.1572</td>
                                        </tr>   
                                        <tr>
                                            <td class="section-border">laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</td>
                                            <td>1,845M</td>
                                            <td class="section-border">mistralai/Mistral-7B-Instruct-v0.1</td>
                                            <td>7B</td>
                                            <td>2.1569</td>
                                        </tr> 
                                        <tr>
                                            <td class="section-border">openai/clip-vit-large-patch14-336</td>
                                            <td>428M</td>
                                            <td class="section-border">mistralai/Mixtral-8x7B-Instruct-v0.1</td>
                                            <td>8x7B</td>
                                            <td>2.0815</td>
                                        </tr> 
                                        <tr>
                                            <td class="section-border">apple/DFN5B-CLIP-ViT-H-14-378</td>
                                            <td>632M</td>
                                            <td class="section-border">lmsys/vicuna-13b-v1.5-16k</td>
                                            <td>13B</td>
                                            <td>2.0443</td>
                                        </tr> 
                                        <tr>
                                            <td class="section-border">laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</td>
                                            <td>1,845M</td>
                                            <td class="section-border">lmsys/vicuna-7b-v1.5-16k</td>
                                            <td>7B</td>
                                            <td>2.0365</td>
                                        </tr> 
                                        <tr>
                                            <td class="section-border">laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</td>
                                            <td>1,845M</td>
                                            <td class="section-border">mistralai/Mixtral-8x7B-Instruct-v0.1</td>
                                            <td>8x7B</td>
                                            <td>1.9889</td>
                                        </tr> 
                                        <tr>
                                            <td class="section-border">openai/clip-vit-large-patch14-336</td>
                                            <td>428M</td>
                                            <td class="section-border">lmsys/vicuna-7b-v1.5</td>
                                            <td>7B</td>
                                            <td>1.9762</td>
                                        </tr> 
                                        <tr>
                                            <td class="section-border">laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</td>
                                            <td>1,845M</td>
                                            <td class="section-border">meta-llama/Llama-2-13b-chat-hf</td>
                                            <td>13B</td>
                                            <td>1.9708</td>
                                        </tr> 
                                        <tr>
                                            <td class="section-border">laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</td>
                                            <td>1,845M</td>
                                            <td class="section-border">lmsys/vicuna-13b-v1.5</td>
                                            <td>13B</td>
                                            <td>1.9412</td>
                                        </tr> 
                                        <tr>
                                            <td class="highlight-red">apple/DFN5B-CLIP-ViT-H-14-378</td>
                                            <td>632M</td>
                                            <td class="highlight-red">lmsys/vicuna-7b-v1.5-16k</td>
                                            <td>7B</td>
                                            <td class="highlight-red">1.8679</td>
                                        </tr>  
                                        </tbody>
                                    </table>
                                </div>
                                <figcaption style="text-align: center; width: 140%;">
                                    Table 1: Final training loss during pretraining stage with original LLaVA pretraining data.
                                </figcaption>
                            </div>
                        </p>
                        <p class="text">
                            <strong>Token merging.</strong> 
                            To increase the throughput of existing ViT models, Token Merging is proposed to gradually combines similar tokens in a transformer to reduce the number of tokens passing through ViT models. 
                            Token Merging has been proven to be effective on image and video classification tasks even without the need for training.
                            We conduct frame-wise token merging in AuroraCap, where the feature is extracted by CLIP ViT-H model. 
                            We show token merging visualization examples from  COCO, VG, SA-1B as follows:
                            <d-figure id="fig-cvcb" >
                                <figure>
                                    <img data-zoomable="" draggable="false" src="static/img/tome_visualize.png" alt="benchmark category">
                                    <figcaption>
                                        <strong>Figure 1:</strong> Token merging visualization. From top to bottom, the image IDs are COCO:COCO-train2014-000000247906, VG:2331508, SA-1B:sa-393200. From left to right, the number of tokens representing the images are 490, 154, 18, and 6.
                                    </figcaption>
                                </figure>
                            </d-figure>
                        </p>
                        </p>
                        <h2 class="text">Training Recipe</h2>
                        <p class="text">
                            <strong>Pretraining stage.</strong> 
                            Similar to LLaVA, we first align visual features with the word embedding space of LLMs. 
                            To achieve this, we freeze the pretrained ViT and LLM, training solely the vision-language connector. 
                            Consistent with LLaVA-1.5, we employ a two-layer MLP as the projection layer and pretrain on 1.3M image-caption pairs.
                        </p>
                        <p class="text">
                            <strong>Vision stage.</strong> 
                            Unlike LLaVA, we next unfreeze the pretrained ViT while freezing the LLM during vision stage and train with the public data among various computer vision tasks (e.g., captioning, object identification, classification, reasoning, VQA, and etc.) to get better generalization.
                            The motivation for doing this is that CLIP ViT usually performs poorly in aspects such as Orientation and Direction, Positional and Relational Context, Quantity and Count.
                            However, since the most of the collected datasets lack high-quality and detailed corresponding language descriptions, the labels often consist of only a few words or a short phrase when converted to text.
                            Therefore, unfreezing the language model at this stage is risky, as it may lead to a degradation in the performance of the language model.
                        </p>
                        <p class="text">
                            <strong>Language stage.</strong> 
                            Finally, we conduct end-to-end training, which means all the components are trainable, with the most high-quality public data during language stage. 
                            We mix all the data, including images and videos, captions and instructions, into each mini-batch for training. 
                            To improve caption performance, we duplicate the image and video captioning datasets twice. We remove all the video training data for image-based model model training.
                        </p>
                        
                        TODO: ADD TRAINING DATASET TABLES

                        <h2 class="text">Evaluation</h2>
                        <p class="text">
                            <strong>Image Captioning.</strong> 
                            We evaluate AuroraCap using CIDEr, BELU-4, BELU-1, METEOR, and ROUGE-L metric on Flickr, NoCaps, and COCO-Cap benchmarks and compare it with LLM-based state-of-the-art methods. 
                            AuroraCap shows good performance under zero-shot settings. 
                            Notice that these benchmarks all contain short captions consisting of a single sentence, so they only partially reflect the model's performance.
                            <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                                <div class="table-container">
                                    <table class="data-table">
                                        <thead>
                                        <tr>
                                            <th class="tb-hdr" rowspan="2">Model</th>
                                            <th colspan="5" class="tb-hdr">Flickr (31,784)</th>
                                            <th colspan="5" class="tb-hdr">NoCaps (4,500)</th>
                                            <th colspan="5" class="tb-hdr">COCO-Cap (5,000)</th>
                                        </tr>
                                        <tr>
                                            <th>C</th>
                                            <th>B@1</th>
                                            <th>B@4</th>
                                            <th>M</th>
                                            <th>R</th>
                                            <th>C</th>
                                            <th>B@1</th>
                                            <th>B@4</th>
                                            <th>M</th>
                                            <th>R</th>
                                            <th>C</th>
                                            <th>B@1</th>
                                            <th>B@4</th>
                                            <th>M</th>
                                            <th>R</th>
                                        </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td class="italic">LLaVA-1.5-7B</td>
                                                <td>74.9</td>
                                                <td>71.7</td>
                                                <td>28.4</td>
                                                <td>26.1</td>
                                                <td>52.8</td>
                                                <td>105.5</td>
                                                <td>82.6</td>
                                                <td>40.2</td>
                                                <td>30.3</td>
                                                <td>59.4</td>
                                                <td>110.3</td>
                                                <td>73.0</td>
                                                <td>29.7</td>
                                                <td>29.2</td>
                                                <td>55.5</td>
                                            </tr>
                                            <tr>
                                                <td>FrozenBiLM</td>
                                                <td>24.7</td>
                                                <td>—</td>
                                                <td>32.2</td>
                                                <td>—</td>
                                                <td>16.8</td>
                                                <td>—</td>
                                                <td>26.8</td>
                                            </tr>
                                            <tr>
                                                <td class="italic">LLaVA-1.5-13B</td>
                                                <td>79.4</td>
                                                <td>73.6</td>
                                                <td>30.2</td>
                                                <td>26.6</td>
                                                <td>53.9</td>
                                                <td>109.2</td>
                                                <td>84.2</td>
                                                <td>42.4</td>
                                                <td class="highlight-red">30.6</td>
                                                <td>60.3</td>
                                                <td>115.6</td>
                                                <td>74.6</td>
                                                <td>31.5</td>
                                                <td class="highlight-red">29.4</td>
                                                <td>56.5</td>
                                            </tr>
                                            <tr>
                                                <td class="italic">LLaVA-1.6-7B</td>
                                                <td>68.4</td>
                                                <td>69.6</td>
                                                <td>26.6</td>
                                                <td>23.2</td>
                                                <td>50.3</td>
                                                <td>88.4</td>
                                                <td>73.8</td>
                                                <td>34.8</td>
                                                <td>25.9</td>
                                                <td>54.6</td>
                                                <td>99.9</td>
                                                <td>67.7</td>
                                                <td>28.4</td>
                                                <td>25.5</td>
                                                <td>52.4</td>
                                            </tr>
                                            <tr>
                                                <td class="italic">LLaVA-1.6-13B</td>
                                                <td>66.6</td>
                                                <td>65.2</td>
                                                <td>24.2</td>
                                                <td>22.2</td>
                                                <td>48.8</td>
                                                <td>88.1</td>
                                                <td>68.7</td>
                                                <td>34.0</td>
                                                <td>25.4</td>
                                                <td>54.9</td>
                                                <td>101.8</td>
                                                <td>62.2</td>
                                                <td>27.5</td>
                                                <td>24.6</td>
                                                <td>52.1</td>
                                            </tr>
                                            <tr>
                                                <td class="italic">MiniCPM-V-3B</td>
                                                <td>66.8</td>
                                                <td>68.0</td>
                                                <td>25.1</td>
                                                <td>27.2</td>
                                                <td>51.0</td>
                                                <td>89.9</td>
                                                <td>79.1</td>
                                                <td>33.2</td>
                                                <td>29.7</td>
                                                <td>55.8</td>
                                                <td>94.2</td>
                                                <td>69.8</td>
                                                <td>23.9</td>
                                                <td>28.3</td>
                                                <td>52.3</td>
                                            </tr>
                                            <tr>
                                                <td class="italic">DeCap</td>
                                                <td>56.7</td>
                                                <td>—</td>
                                                <td>21.2</td>
                                                <td>21.8</td>
                                                <td>—</td>
                                                <td>42.7</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>91.2</td>
                                                <td>—</td>
                                                <td>24.7</td>
                                                <td>25.0</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td class="italic">Flamingo-80B</td>
                                                <td>67.2</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>84.3</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td class="italic">Chameleon-34B</td>
                                                <td>74.7<sup>2</sup></td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>120.2<sup>2</sup></td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td class="italic">GPT-4V</td>
                                                <td>55.3<sup>8</sup></td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>78.5<sup>8</sup></td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td class="italic">Gemini-1.5 Pro</td>
                                                <td>82.2<sup>4</sup></td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>99.8<sup>2</sup></td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td><strong>AuroraCap-7B</strong></td>
                                                <td class="highlight-red"><strong>88.9</strong></td>
                                                <td class="highlight-red"><strong>75.6</strong></td>
                                                <td class="highlight-red"><strong>32.8</strong></td>
                                                <td class="highlight-red"><strong>26.7</strong></td>
                                                <td class="highlight-red"><strong>55.4</strong></td>
                                                <td class="highlight-red"><strong>111.4</strong></td>
                                                <td class="highlight-red"><strong>85.6</strong></td>
                                                <td class="highlight-red"><strong>44.4</strong></td>
                                                <td>29.9</td>
                                                <td class="highlight-red"><strong>60.6</strong></td>
                                                <td class="highlight-red"><strong>120.8</strong></td>
                                                <td class="highlight-red"><strong>78.0</strong></td>
                                                <td class="highlight-red"><strong>35.3</strong></td>
                                                <td>28.6</td>
                                                <td class="highlight-red"><strong>57.2</strong></td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                                <figcaption style="text-align: center; width: 140%;">
                                    Table 2: Comparison AuroraCap with SoTA methods on video question answering and classification benchmarks under zero-shot setting. The model size is 7B by default.
                                </figcaption>
                            </div>
                        </p>
                        <p class="text">
                            <strong>Video Captioning.</strong> 
                            Although the current video captioning benchmarks are only contains one-sentence captions, to compare with prior work, we similarly evaluate on these benchmarks. 
                            We evaluate AuroraCap on MSR-VTT, VATEX, and ActivityNet Captions and compare it with other methods.
                            <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                                <div class="table-container">
                                    <table class="data-table">
                                        <thead>
                                        <tr>
                                            <th class="tb-hdr" rowspan="2">Model</th>
                                            <th colspan="5" class="tb-hdr">MSR-VTT (1,000)</th>
                                            <th colspan="5" class="tb-hdr">VATEX (1,000)</th>
                                        </tr>
                                        <tr>
                                            <th>C</th>
                                            <th>B@1</th>
                                            <th>B@4</th>
                                            <th>M</th>
                                            <th>R</th>
                                            <th>C</th>
                                            <th>B@1</th>
                                            <th>B@4</th>
                                            <th>M</th>
                                            <th>R</th>
                                        </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>ZeroCap</td>
                                                <td>9.6</td>
                                                <td>—</td>
                                                <td>2.9</td>
                                                <td>16.3</td>
                                                <td>35.4</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>DeCap</td>
                                                <td>18.6</td>
                                                <td>—</td>
                                                <td>14.7</td>
                                                <td>20.4</td>
                                                <td>—</td>
                                                <td>18.7</td>
                                                <td>—</td>
                                                <td>13.1</td>
                                                <td>15.3</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>PaLI-3</td>
                                                <td>21.3</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>Ma et al.</td>
                                                <td>22.1</td>
                                                <td>—</td>
                                                <td>3.5</td>
                                                <td>17.3</td>
                                                <td>28.7</td>
                                                <td>23.9</td>
                                                <td>—</td>
                                                <td>2.8</td>
                                                <td>14.1</td>
                                                <td>23.5</td>
                                            </tr>
                                            <tr>
                                                <td>LLaVA-7B</td>
                                                <td>16.9</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>Video-LLAMA</td>
                                                <td>2.3</td>
                                                <td>—</td>
                                                <td>4.9</td>
                                                <td>16.8</td>
                                                <td>—</td>
                                                <td>3.8</td>
                                                <td>—</td>
                                                <td>4.3</td>
                                                <td>16.3</td>
                                                <td>21.8</td>
                                            </tr>
                                            <tr>
                                                <td>AuroraCap-7B</td>
                                                <td class="highlight-red">33.1</td>
                                                <td class="highlight-red">58.6</td>
                                                <td class="highlight-red">21.0</td>
                                                <td class="highlight-red">23.9</td>
                                                <td class="highlight-red">49.5</td>
                                                <td class="highlight-red">33.8</td>
                                                <td class="highlight-red">57.1</td>
                                                <td class="highlight-red">18.4</td>
                                                <td class="highlight-red">19.0</td>
                                                <td class="highlight-red">40.8</td>
                                            </tr>

                                        </tbody>
                                    </table>
                                </div>
                                <figcaption style="text-align: center; width: 140%;">
                                    Table 3: Comparison AuroraCap with SoTA methods on existing video captioning benchmarks under zero-shot setting.
                                </figcaption>
                            </div>
                        </p>
                        <p class="text">
                            <strong>Video Question Answering.</strong> 
                            We evaluate AuroraCap on MSVD-QA, ActivityNet-QA, MSRVTT-QA, and iVQA for video question answering tasks. 
                            Although AuroraCap is primarily a captioning model, it achieves competitive performance in these tasks. 
                            Despite token limits, AuroraCap generates longer answers than the ground truth, leading to lower scores.

                            <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                                <div class="table-container">
                                    <table class="data-table">
                                        <thead>
                                        <tr>
                                            <th class="tb-hdr">Model</th>
                                            <th colspan="2" class="tb-hdr">ANet</th>
                                            <th colspan="2" class="tb-hdr">MSVD</th>
                                            <th colspan="2" class="tb-hdr">MSR-VTT</th>
                                            <th class="tb-hdr">iVQA</th>
                                        </tr>
                                        <tr>
                                            <th></th>
                                            <th>Acc</th>
                                            <th>Score</th>
                                            <th>Acc</th>
                                            <th>Score</th>
                                            <th>Acc</th>
                                            <th>Score</th>
                                            <th>Acc</th>
                                        </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>Just Ask</td>
                                                <td colspan="2">—</td>
                                                <td colspan="2">—</td>
                                                <td colspan="2">—</td>
                                                <td>12.2</td>
                                            </tr>
                                            <tr>
                                                <td>FrozenBiLM</td>
                                                <td>24.7</td>
                                                <td>—</td>
                                                <td>32.2</td>
                                                <td>—</td>
                                                <td>16.8</td>
                                                <td>—</td>
                                                <td>26.8</td>
                                            </tr>
                                            <tr>
                                                <td>Video-LLAMA</td>
                                                <td>12.4</td>
                                                <td>1.1</td>
                                                <td>51.6</td>
                                                <td>2.5</td>
                                                <td>29.6</td>
                                                <td>1.8</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>VideoChat</td>
                                                <td>26.5</td>
                                                <td>2.2</td>
                                                <td>56.3</td>
                                                <td>2.8</td>
                                                <td>45.0</td>
                                                <td>2.5</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>Video-ChatGPT</td>
                                                <td>35.2</td>
                                                <td>2.7</td>
                                                <td>64.9</td>
                                                <td>3.3</td>
                                                <td>49.3</td>
                                                <td>2.8</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>LLAMA-VID</td>
                                                <td>47.4</td>
                                                <td>3.3</td>
                                                <td>69.7</td>
                                                <td>3.7</td>
                                                <td>57.7</td>
                                                <td>3.2</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>Video-LLAVA</td>
                                                <td>45.3</td>
                                                <td>3.3</td>
                                                <td>70.7</td>
                                                <td>3.9</td>
                                                <td>59.2</td>
                                                <td>3.5</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>FreeVA</td>
                                                <td>51.2</td>
                                                <td>3.5</td>
                                                <td>73.8</td>
                                                <td><span class="highlight-red">4.1</span></td>
                                                <td><span class="highlight-red">60.0</strong></td>
                                                <td><span class="highlight-red">3.5</strong></td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>LLAVA-NeXT-Video</td>
                                                <td>53.5</td>
                                                <td>3.2</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>MovieChat</td>
                                                <td>45.7</td>
                                                <td>3.4</td>
                                                <td>75.2</td>
                                                <td>3.8</td>
                                                <td>52.7</td>
                                                <td>2.6</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td>MovieChat+</td>
                                                <td>48.1</td>
                                                <td>3.4</td>
                                                <td><span class="highlight-red">76.5</span></td>
                                                <td>3.9</td>
                                                <td>53.9</td>
                                                <td>2.7</td>
                                                <td>—</td>
                                            </tr>
                                            <tr>
                                                <td><strong>AuroraCap-7B</strong></td>
                                                <td class="highlight-red"><strong>61.8</strong></td>
                                                <td class="highlight-red"><strong>3.8</strong></td>
                                                <td>62.6</td>
                                                <td>3.6</td>
                                                <td>43.5</td>
                                                <td>2.9</td>
                                                <td class="highlight-red"><strong>55.2</strong></td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                                <figcaption style="text-align: center; width: 140%;">
                                    Table 4: Comparison AuroraCap with SoTA methods on video question answering and classification benchmarks under zero-shot setting. The model size is 7B by default.
                                </figcaption>
                            </div>
                        </p>
            </div>
            
            <!-- <div id="sec:benchmarking" class="sub-section">
                <h1 class="text">Analyzing the Benchmarks</h1>

                    <p class="text">
                        <p class="text">
                        <strong>Who's answering: LLM or MLLM?:</strong> We compare performance between vision-disabled and vision-enabled settings across MLLMs trained with 23 different vision backbones. Our findings reveal that some benchmarks such as MMMU and AI2D are less reliant on visual inputs, whereas others such as MMVP and MME experience significant performance declines, indicating their effective evaluation of multimodality</li>
                        </p>
                        <p class="text">
                            <strong>Benchmark Clustering and Analysis:</strong> Through correlation analysis and principal component analysis of MLLM performances across various benchmarks, distinct clusters emerge categorized as "General," "Knowledge," "Chart & OCR," and "Vision-Centric." 
                            We also find that vision-centric benchmarks are underrepresented in the current evaluation landscape.
                        </p>
                    <d-figure id="fig-comparison" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/bench_cat.png" alt="benchmark category">
                            <figcaption>
                                <strong>Figure 1:</strong> Analyzing the benchmarks.
                                <strong>Left:</strong> Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. <strong>Right:</strong> Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size.
                            </figcaption>
                        </figure>
                    </d-figure>
            </div>
            <div id="cv-bench" class="sub-section">

                    <p class="text"><strong>Cambrian Vision-Centric Benchmark (CV-Bench) </strong>
                        To address the scarcity of vision-centric benchmarks, we introduce CV-Bench&mdash;repurposing standard vision tasks for multimodal evaluation. CV-Bench contains approximately 2600 vision-centric VQA questions, addressing the issues with existing vision-centric bechmark size.
                    </p>

                    <d-figure id="fig-cvcb" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/cvcb.jpg" alt="benchmark category">
                            <figcaption>
                                <strong>Figure 2:</strong> Example questions in CV-Bench that focuses on 2D and 3D visual understanding.
                            </figcaption>
                        </figure>
                    </d-figure>

            </div>
            <div id="sec:inst_tuning" class="sub-section">

            <h1 class="text">Instruction Tuning Recipes </h1>
                <p class="text">
                    MLLMs connect pre-trained LLM and vision backbones using a connector such as an MLP projector. Various studies have suggested different optimal training methodologies for MLLMs.
                </p>
                <p class="text">
                    <strong>One Stage vs Two Stage Training</strong> Recent work suggests skipping connector pre-training to reduce compute costs without harming performance. 
                    We experiment with 0, 0.5M, and 1.2M adapter data. Following LLaVA's method<d-cite key="liu2023visual"></d-cite>, we initially tune only the connector, then unfreeze both the LLM and connector for instruction tuning with a 737K data mix. <a href="#fig-studyadapter">Figure 3</a> indicates that pre-training the connector boosts performance, and using more adapter data enhances it further, leading us to standardize on a 2-stage training approach with 1.2M adapter data.
                </p>
                
                <p class="text">
                    <strong>Freeze vs Unfreeze Vision Encoder</strong>
                    There are also mixed practices in freezing or unfreezing vision backbones during fine-tuning. 
                    Some argue that unfreezing the vision backbone significantly degrades performance. 
                    Our experiments demonstrate that, with a reasonable vision model learning rate, 
                    unfreezing benefits performance across all benchmarks except for a marginal change in Knowledge benchmarks.
                </p>
                <d-figure id="fig-studyadapter" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/performance_plot.png" alt="Instruction Tuning Recipes">
                        <figcaption>
                            <strong>Figure 3:</strong> MLLMs benefit from pre-training the adapter with more data, and finetuning with unfrozen visual encoder.
                        </figcaption>
                    </figure>
                </d-figure>
            </div>
            <div id='training-recipe' class="viusal-representation-block">
            <h1 class="text">MLLMs as a Vision Model Evaluator </h1>
                
                <p class="text">
                    MLLMs provide a more real-world evaluation of visual representations than traditional benchmarks like ImageNet-1k. We use 2-stage instruction tuning with 1.2M adapter data and 737K fine-tuning data to compare a variety of vision models on downstream MLLM performance.
                    Our evaluations show language-supervised models exhibit strong advantages across all benchmark categories, especially in OCR & chart tasks. However, despite the smaller dataset size of SSL models like DINOv2, they perform competitively in vision-centric benchmarks. 
                </p>

                
                <d-figure id="fig-mllm_as_interface">
                    <figure class="responsive-content">
                        <iframe src="static/img/tuning_recipes_plot.html"></iframe>
                        <img data-zoomable="" draggable="false" src="static/img/mllm_interface_shared.png" alt="MLLMs as an interface to evaluate visual representations">
                        <figcaption>
                            <strong>Figure 4:</strong> MLLMs as an interface to evaluate visual representations.
                        </figcaption>
                    </figure>
                    <p class="click-hint" style="width: 85%; margin-top: -2em;" id="mllm_interface_click_hint">
                        <img src="static/img/icons/click.gif" style="width: 1.5rem">
                        <strong>Hover & click to interact.</strong>
                    </p>                
                </d-figure>
                <p class="text">
                    <strong>Narrowing the gap between CLIP and SSL models</strong> 
                    Above, we observe that DINOv2 stands midway between SSL models and CLIP models on general VQA and knowledge VQA tasks, 
                    even outperforming some CLIP models on vision-centric benchmarks with higher resolution. 
                    We investigate unfreezing the vision backbones and increasing the amount of visual fine-tuning data to narrow this gap. 
                    In <a href="#fig-narrowgap">Figure 5</a>, we observe that by unfreezing the vision backbone, 
                    the DINOv2-based MLLM fine-tuned with 5M data surpasses the MLLM trained with a CLIP model on 0.7M data. 
                    Additionally, the gap between DINOv2 and the CLIP models is reduced under the 5M data experiment setting.
                </p>
                <d-figure id="fig-narrowgap">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/narrow_gap.png" alt="Narrowing the gap between CLIP and SSL models">
                        <figcaption>
                            <strong>Figure 5:</strong> By unfreezing the visual backbone and fine-tuning on 5M examples, the gap between CLIP and DINOv2 can be narrowed.
                        </figcaption>
                    </figure>
                </d-figure>
            
                <p class="text">
                    <strong>Combining Multiple Vision Encoders </strong>
                    As observed in <a href="#fig-mllm_as_interface">Figure 4</a>, different vision models excel in different aspects of MLLM performance. 
                    We explore the potential of combining multiple vision encoders to leverage their distinctive representations.
                    Given that different vision encoders use varying architectures and image resolutions, we interpolate the output visual tokens to a fixed number, 576.
                    The results are tabulated in <a href="#tab:model_ensemble">Table 2</a>, where we observe consistent performance improvements with the addition of more models.
                </p>

                <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                        <table class="data-table">
                            <thead>
                            <tr>
                                <th colspan="1" class="tb-hdr">Vision Backbone</th>
                                <th colspan="1" class="tb-hdr"></th>
                                <th colspan="4" class="tb-hdr">General</th>
                                <th colspan="4" class="tb-hdr">Knowledge</th>
                                <th colspan="4" class="tb-hdr">OCR & Chart</th>
                                <th colspan="4" class="tb-hdr">Vision-Centric</th>
                            </tr>
                            <tr>
                                <th class="section-border">Encoders</th>
                                <th class="section-border"><b>Average</b></th>
                                <th>MME<sup>P</sup></th>
                                <th>MMB</th>
                                <th>SEED<sup>I</sup></th>
                                <th class="section-border">GQA</th>
                                <th>SQA<sup>I</sup></th>
                                <th>MMMU<sup>V</sup></th>
                                <th>MathVista<sup>M</sup></th>
                                <th class="section-border">AI2D</th>
                                <th>ChartQA</th>
                                <th>OCRBench</th>
                                <th>TextVQA</th>
                                <th class="section-border">DocVQA</th>
                                <th>MMVP</th>
                                <th>RealWorldQA</th>
                                <th>CV-Bench<sup>2D</sup></th>
                                <th>CV-Bench<sup>3D</sup></th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td class="section-border">SigLIP+DINOv2</td>
                                <td class="section-border">51.61</td>
                                <td>1,432.02</td>
                                <td>61.28</td>
                                <td>65.99</td>
                                <td class="section-border">63.30</td>
                                <td>68.82</td>
                                <td>35.69</td>
                                <td>29.40</td>
                                <td class="section-border">60.01</td>
                                <td>43.00</td>
                                <td>35.70</td>
                                <td>60.40</td>
                                <td class="section-border">37.54</td>
                                <td>30.00</td>
                                <td>53.99</td>
                                <td>55.52</td>
                                <td>53.58</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+DINOv2+ConvNext</td>
                                <td class="section-border">54.52</td>
                                <td>1,503.51</td>
                                <td>63.83</td>
                                <td>67.97</td>
                                <td class="section-border">63.95</td>
                                <td>70.40</td>
                                <td class="highlight">35.99</td>
                                <td>29.30</td>
                                <td class="section-border">60.69</td>
                                <td>48.20</td>
                                <td>36.90</td>
                                <td>64.97</td>
                                <td class="section-border">45.53</td>
                                <td class="highlight">34.67</td>
                                <td>58.69</td>
                                <td>55.74</td>
                                <td>60.33</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+DINOv2+ConvNext+CLIP</td>
                                <td class="section-border highlight">54.74</td>
                                <td>1,479.46</td>
                                <td>63.32</td>
                                <td>67.63</td>
                                <td class="section-border highlight">64.04</td>
                                <td class="highlight">71.39</td>
                                <td>35.49</td>
                                <td>29.10</td>
                                <td class="section-border">59.88</td>
                                <td>50.24</td>
                                <td class="highlight">39.60</td>
                                <td>64.55</td>
                                <td class="section-border">46.12</td>
                                <td>32.67</td>
                                <td class="highlight">58.95</td>
                                <td>58.54</td>
                                <td class="highlight">60.42</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+ConvNext</td>
                                <td class="section-border">54.53</td>
                                <td>1,494.97</td>
                                <td class="highlight">64.60</td>
                                <td>67.98</td>
                                <td class="section-border">63.58</td>
                                <td>71.05</td>
                                <td>34.90</td>
                                <td>29.80</td>
                                <td class="section-border">60.85</td>
                                <td>50.64</td>
                                <td>38.00</td>
                                <td>64.53</td>
                                <td class="section-border">46.52</td>
                                <td>32.00</td>
                                <td>57.91</td>
                                <td class="highlight">58.83</td>
                                <td>56.58</td>
                            </tr>
                            <tr>
                                <td class="section-border">CLIP+ConvNext</td>
                                <td class="section-border">54.45</td>
                                <td class="highlight">1,511.08</td>
                                <td>63.83</td>
                                <td>67.41</td>
                                <td class="section-border">63.63</td>
                                <td>70.80</td>
                                <td>35.09</td>
                                <td>30.40</td>
                                <td class="section-border">59.91</td>
                                <td>51.32</td>
                                <td>35.00</td>
                                <td>64.45</td>
                                <td class="section-border">47.88</td>
                                <td>33.33</td>
                                <td>57.25</td>
                                <td>56.32</td>
                                <td>59.08</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+DINOv2+ConvNext</td>
                                <td class="section-border">53.78</td>
                                <td>1,450.64</td>
                                <td>63.57</td>
                                <td>67.79</td>
                                <td class="section-border">63.63</td>
                                <td>71.34</td>
                                <td>34.80</td>
                                <td>30.20</td>
                                <td class="section-border highlight">61.04</td>
                                <td>49.32</td>
                                <td>37.70</td>
                                <td>64.05</td>
                                <td class="section-border">45.83</td>
                                <td>30.00</td>
                                <td>56.21</td>
                                <td>58.08</td>
                                <td>54.33</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+CLIP+ConvNext</td>
                                <td class="section-border">54.53</td>
                                <td>1,507.28</td>
                                <td>63.23</td>
                                <td class="highlight">68.64</td>
                                <td class="section-border">63.63</td>
                                <td>71.10</td>
                                <td>35.89</td>
                                <td class="highlight">30.90</td>
                                <td class="section-border">59.97</td>
                                <td class="highlight">52.36</td>
                                <td>38.50</td>
                                <td class="highlight">65.40</td>
                                <td class="section-border highlight">47.92</td>
                                <td>28.67</td>
                                <td>57.25</td>
                                <td>57.66</td>
                                <td>55.92</td>
                            </tr>
                            </tbody>
                        </table>
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Table 2: All Benchmark Results for Model Ensemble with 1.2M Adapter Data + 737K
                        Instruction Tuning Data
                    </figcaption>
                </div>
            
                <p class="text">
                    However, this strategy has two limitations:
                    1) it employs interpolation, which can potentially lead to information loss, especially on vision encoders with high-resolution feature maps, and 
                    2) it treats each model equally by simple concatenation. 
                    Therefore, we seek a more effective strategy that fully leverages model combinations with less information loss and more flexibility.
                </p>

            </div> -->
        </div>

        <div id='vdc' class="connector-block">

            <h1 class="text">VDC: A Video Detailed Captioning Benchmark</h1>
            <p class="text">
                <h2 class="text">Benchmark Dataset Curation</h2>
                <ol class="text">
                    <li><strong>Video collection and processing.</strong>
                        We building VDC upon Panda-70M, Ego4D, Mixkit, Pixabay, and Pexels. 
                        Note that the videos used in VDC construction are not included in the training data of AuroraCap.
                        To ensure balanced data distribution, we allocate equal proportions of different video sources.  
                        We first split the video into clips and apply dense frame extraction, then manually replacing blurry frames with adjacent clear ones.
                    </li>
                    <li><strong>Structured detailed captions construction pipeline.</strong>
                        We develop a structured detailed captions construction pipeline to generate extra detailed descriptions from various perspectives, significantly extending the length and enhancing the richness compared to previous benchmarks. 
                        The structured captions in VDC encompass not only <strong>short</strong> and <strong>detailed captions</strong> but also three additional categories: (1) <strong>main object caption</strong>; (2) <strong>background caption</strong>; and (3) <strong>camera caption</strong>.
                        To generate detailed, fine-grained, and accurate captions, we leverage <strong>GPT-4o</strong> to produce video descriptions. 
                        We observed that generating all captions in a single conversation round often introduces hallucinations in the detailed captions. 
                        To address this, we design a hierarchical prompt strategy to efficiently obtain accurate structured captions and detailed captions in two conversation rounds: (1) <strong>Structured Captions Generation</strong> and (2) <strong>Detailed Captions Integration</strong>. 
                    </li>
                    <li><strong>Comparison on numerical statistics.</strong>
                        The visual representation in Figure 2 demonstrates the video duration distribution of VDC and the length distribution of structured captions in VDC. 
                        <d-figure id="fig-vision_connector">
                            <figure>
                                <img data-zoomable="" draggable="false" src="static/img/vdc.png" alt="Video length in VDC" style="width: 80%;display: block; margin: 0 auto;">
                                <figcaption>
                                    <strong>Figure 2:</strong> Video length in VDC and distribution of structured caption length.
                                </figcaption>
                            </figure>
                        </d-figure>
                        As illustrated in Table 5, the average length of detailed descriptions in~\benchmark~is significantly longer than in previous benchmarks. 
                        <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                            <div class="table-container">
                                <table class="data-table">
                                    <thead>
                                    <tr>
                                        <th class="tb-hdr">Dataset</th>
                                        <th>Theme</th>
                                        <th># Video</th>
                                        <th># Clip</th>
                                        <th># Caption</th>
                                        <th># Word</th>
                                        <th># Vocab.</th>
                                        <th>Ave. Length</th>
                                    </tr>
                                    <tr>
                                        <td class="italic">MSVD</td>
                                        <td>Open</td>
                                        <td>1,970</td>
                                        <td>1,970</td>
                                        <td>70,028</td>
                                        <td>607,339</td>
                                        <td>13,010</td>
                                        <td>8.67</td>
                                    </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td class="italic">MSR-VTT</td>
                                            <td>Open</td>
                                            <td>7,180</td>
                                            <td>10,000</td>
                                            <td>200,000</td>
                                            <td>1,856,523</td>
                                            <td>29,316</td>
                                            <td>9.28</td>
                                        </tr>
                                        <tr>
                                            <td class="italic">ActivityNet</td>
                                            <td>Open</td>
                                            <td>20,000</td>
                                            <td>100,000</td>
                                            <td>100,000</td>
                                            <td>1,340,000</td>
                                            <td>15,564</td>
                                            <td>13.40</td>
                                        </tr>
                                        <tr>
                                            <td class="italic">S-MiT</td>
                                            <td>Open</td>
                                            <td>515,912</td>
                                            <td>515,912</td>
                                            <td>515,912</td>
                                            <td>5,618,064</td>
                                            <td>50,570</td>
                                            <td>10.89</td>
                                        </tr>
                                        <tr>
                                            <td class="italic">M-VAD</td>
                                            <td>Movie</td>
                                            <td>92</td>
                                            <td>48,986</td>
                                            <td>55,905</td>
                                            <td>519,933</td>
                                            <td>18,269</td>
                                            <td>9.30</td>
                                        </tr>
                                        <tr>
                                            <td class="italic">MPII-MD</td>
                                            <td>Movie</td>
                                            <td>94</td>
                                            <td>68,337</td>
                                            <td>68,375</td>
                                            <td>653,467</td>
                                            <td>24,549</td>
                                            <td>9.56</td>
                                        </tr>
                                        <tr>
                                            <td class="italic">Youcook2</td>
                                            <td>Cooking</td>
                                            <td>2,000</td>
                                            <td>15,400</td>
                                            <td>15,400</td>
                                            <td>121,418</td>
                                            <td>2,583</td>
                                            <td>7.88</td>
                                        </tr>
                                        <tr>
                                            <td class="italic">Charades</td>
                                            <td>Human</td>
                                            <td>9,848</td>
                                            <td>10,000</td>
                                            <td>27,380</td>
                                            <td>607,339</td>
                                            <td>13,000</td>
                                            <td>22.18</td>
                                        </tr>
                                        <tr>
                                            <td class="italic">VATEX</td>
                                            <td>Open</td>
                                            <td>41,300</td>
                                            <td>41,300</td>
                                            <td>413,000</td>
                                            <td>4,994,768</td>
                                            <td>44,103</td>
                                            <td>12.09</td>
                                        </tr>
                                        <tr>
                                            <td class="italic">VDC (ours)</td>
                                            <td>Open</td>
                                            <td>1,027</td>
                                            <td>1,027</td>
                                            <td>1,027</td>
                                            <td>515,441</td>
                                            <td>20,419</td>
                                            <td class="highlight-red">500.91</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                            <figcaption style="text-align: center; width: 140%;">
                                Table 5: Benchmark comparison for video captioning task. Ave. Length indicates the average number of words per caption. 
                            </figcaption>
                        </div>

                    </li>
                </ol>
            </p>

        </div>

        <div id="instruction_data" class="data-block">
            <h1 class="text">Instruction Tuning Data for Training MLLMs</h1>
            <p class="text">
                Previous work highlights the importance of data in training MLLMs, but explicit investigations are limited. 
                In this study, we gather all available instruction tuning data and examine data curation by enhancing diversity, balancing sources, and improving mixtures. 
                
            </p>
    
            <div class="subsection">
                <h3 class="text">Data Collection</h3>
                <p class="text" id="data_collection">
                    <strong>Collecting Instruction Tuning Data from existing data sources</strong> 
                    We first use existing multimodal benchmarks and datasets involving visual interaction data,
                    such as Visual Question Answering (VQA) and OCR data. 
                    We also collect a small volume of high-quality language-only instruction-following data to maintain its language ability. 
                </p>
                <d-figure id="fig-cambrian7m">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/cambrian_7m.png" alt="Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for Training MLLM">
                        <figcaption>
                            <strong>Figure 7:</strong> Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for Training MLLM.
                        </figcaption>
                    </figure>
                </d-figure>
                <p class="text">
                    <strong>Targeted Internet Data Collection Engine</strong> 
                    We also introduce a data engine designed to create large-scale, reliable, 
                    high-quality knowledge-based multimodal instruction tuning data.
                </p>
                <d-figure id="fig-dataengine">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/dataenginefigurepdf_crop.png" alt="Targeted Internet Data Collection Engine">
                        <figcaption>
                            <strong>Figure 8:</strong> Targeted Internet Data Collection Engine.
                        </figcaption>
                    </figure>
                </d-figure>
    
                <p class="text">
                    <strong>Cambrian-10M</strong> 
                    To this end, we create a large pool of instruction tuning data, which we refer to as Cambrian-10M. 
                    This pool contains approximately 9784k data points, offering a diverse range of data for our work and future research. 
                    We visualize its composition in <a href="#fig-cambrian7m">Figure 7</a>.
                </p>
            </div>
    
            <div id="sec:data_curation" class="subsection">
                <h3 class="text">Data Curation</h3>
                <p class="text">
                    Cambrian-10M is a large pool of instruction tuning data sourced from a variety of data sources, 
                    with an unbalanced data ratio between categories. 
                    Here, we take a preliminary step to study data curation by improving data balancing and adjusting data ratios.
                </p>
    
                <p class="text" id="data_curation">
                    <strong>Data Balancing</strong> 
                    We follow previous work to set thresholds t 
                    for the number of data points from a single data source. 
                    We choose t = 150k, 250k, 350k, and 450k in this section and observe an 
                    elbow effect in <a href="#tab:data_balance_result">Table 3</a>&mdash;finding that a threshold between 250k and 350k work the best for Cambrian-10M.
                </p>
                <d-figure id="fig-filter_k">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/Cumulative_Sum_of_Counts.png" alt="Data Balancing via Applying Thresholds on Data Sources">
                        <figcaption>
                            <strong>Figure 9:</strong> Data Balancing via Applying Thresholds on Data Sources.
                        </figcaption>
                    </figure>
                </d-figure>
                <br>
                <div id="tab:data_balance_result" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                        <table class="data-table">
                          <thead>
                            <tr>
                              <th></th>
                              <th>Average</th>
                              <th>General</th>
                              <th>Knowledge</th>
                              <th>OCR & Chart</th>
                              <th>Vision-Centric</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>150k</td>
                              <td>53.7</td>
                              <td>68.0</td>
                              <td>51.3</td>
                              <td>45.2</td>
                              <td>50.5</td>
                            </tr>
                            <tr>
                              <td>250k</td>
                              <td class="highlight">54.3</td>
                              <td class="highlight">68.1</td>
                              <td>51.5</td>
                              <td>45.3</td>
                              <td>52.2</td>
                            </tr>
                            <tr>
                              <td>350k</td>
                              <td class="highlight">54.3</td>
                              <td>67.4</td>
                              <td>51.4</td>
                              <td class="highlight">46.0</td>
                              <td class="highlight">52.3</td>
                            </tr>
                            <tr>
                              <td>450k</td>
                              <td>54.2</td>
                              <td>68.0</td>
                              <td class="highlight">52.2</td>
                              <td>45.5</td>
                              <td>50.7</td>
                            </tr>
                          </tbody>
                        </table>
                      </div>

                    <figcaption style="text-align: center; width: 100%;">
                        <strong>Table 3:</strong> Threshold 𝑡 value between 250k and 350k obtains better performance.
                    </figcaption>
                </div>

                <p class="text">
                    <strong>Data Ratio</strong> 
                    Given the various capabilities of different types of visual instruction tuning data, it is essential to balance the ratio of these data types. 
                    We conduct pilot experiments with a fixed dataset size of 1350k, 
                    examining the impact of different data ratios on downstream performance. 
                    We visualize the results in <a href="#fig-data_ratio">Figure 10</a> and summarize our findings as follows: 
                    (i) Balancing General, OCR, and Language data is crucial. 
                    (ii) Performance on knowledge-intensive tasks is influenced by multiple factors, 
                    often requiring a mix of OCR, chart, reasoning, and general perception. 
                </p>
                <d-figure id="fig-data_ratio">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/data_mixture_ratio_w_avg_score.png" alt="Exploring instruction tuning data mixture ratios">
                        <figcaption>
                            <strong>Figrue 10:</strong> Exploring instruction tuning data mixture ratios.
                        </figcaption>
                    </figure>
                </d-figure>

                <p class="text">
                    <strong>Cambrian-7M</strong> 
                    By applying data filtering to Cambrian-10M with our identified data ratio, we create a smaller but higher-quality dataset called Cambrian-7M.
                    <a href="#tab:data_ratio_result">Table 4</a> showcases the benefits of a well-balanced and carefully curated dataset. Despite having fewer samples, Cambrian-7M demonstrates improved performance.
                </p>
                <div id="tab:data_ratio_result" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                        <table class="data-table">
                          <thead>
                            <tr>
                              <th></th>
                              <th>Average</th>
                              <th>General</th>
                              <th>Knowledge</th>
                              <th>OCR & Chart</th>
                              <th>Vision-Centric</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>LLaVA-665K</td>
                              <td>40.7</td>
                              <td>64.7</td>
                              <td>45.2</td>
                              <td>20.8</td>
                              <td>32.0</td>
                            </tr>
                            <tr>
                              <td>Cambrian-10M</td>
                              <td>54.8</td>
                              <td>68.7</td>
                              <td>51.6</td>
                              <td class="highlight">47.3</td>
                              <td>51.4</td>
                            </tr>
                            <tr>
                              <td>Cambrian-7M</td>
                              <td class="highlight">55.9</td>
                              <td class="highlight">69.6</td>
                              <td class="highlight">52.6</td>
                              <td class="highlight">47.3</td>
                              <td class="highlight">54.1</td>
                            </tr>
                          </tbody>
                        </table>
                      </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Table 4: Performance improves with better instruction tuning data curation.
                    </figcaption>
                </div>
                

            </div>
    
            <div class="subsection">
                <h3 class="text">Alleviating the "Answer Machine Phenomenon" via System Prompts</h3>
                <p class="text">
                    Here, we investigate a phenomenon we term the "answer machine phenomenon." 
                    We observe that a well-trained MLLM may excel at VQA benchmarks, but lack basic conversational abilities and default to outputting short, curt responses (see examples in <a href="#fig-sysprompt">Figure 5</a>). 
                </p>
    
                <p class="text">
                    To address this, we find that incorporating additional system prompts during training mitigates this phenomenon. 
                    We append prompts such as "<em>Answer the question using a single word or phrase.</em>" 
                    before questions that generate a single word or phrase in the response. 
                    We observe that after integrating these system prompts, the model's benchmark performance remains unchanged, 
                    while its conversational ability significantly improves. 
                </p>
                <d-figure id="fig-sysprompt">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/sysprompt.jpg" alt="Incorporating System Prompt in Instruction Tuning Data alleviates “Answer Machine Phenomenon”">
                        <figcaption>
                            <strong>Figure 11:</strong> Incorporating System Prompt in Instruction Tuning Data alleviates the “Answer Machine Phenomenon”.
                        </figcaption>
                    </figure>
                </d-figure>
            </div>
        </div>

        <div id='sota' class="sota-block">
            <h1 class="text">State of the Art MLLM Performance</h1>
            <p class="text">
                Finally, we leverage the insights from all of our previous studies to train a high-performance Cambrian model.
                We train with three different sizes of LLM backbones: LLaMA-3-Instruct-8B, Vicuna-1.5-13B, and Hermes-2-Yi-34B.
                Our visual tower uses a combination of four models&mdash;SigLIP, CLIP, DINOv2, and OpenCLIP ConvNeXt 
                (see <a href="#sec:model_ensemble">Combining Multiple Vision Encoders</a>) with the <a href="#connector_design">Spatial Vision Aggregator</a>.
                We use 2.5M adapter data and Cambrian-7M instruction tuning data (see <a href="#sec:data_curation">Data Curation</a>).
                We evaluate our models on the <a href="#sec:benchmarking">categorized benchmarks</a>, and tabulate the results in <a href="#tab:final_table">Table 5</a>. Cambrian-1 exceeds other open-source models such as LLaVA-NeXT and Mini-Gemini, and achieves comparable performance on a number of benchmarks with the best proprietary models such as GPT-4V, Gemini-Pro, and MM-1.
            </p>
            <div id="tab:final_table" style="display: flex; flex-direction: column; align-items: center;" class="figure">
                <div class="table-container">
                    <table class="data-table">
                        <thead>
                        <tr>
                            <th colspan="2" class="tb-hdr">Model</th>
                            <th colspan="5" class="tb-hdr">General</th>
                            <th colspan="5" class="tb-hdr">Knowledge</th>
                            <th colspan="5" class="tb-hdr">OCR & Chart</th>
                            <th colspan="5" class="tb-hdr">Vision-Centric</th>
                        </tr>
                        <tr>
                            <th>Method</th>
                            <th class="rotate"># Vis Tok.</th>
                            <th class="rotate">Avg</th>
                            <th class="rotate">MME<sup>P</sup></th>
                            <th class="rotate">MMB</th>
                            <th class="rotate">SEED<sup>I</sup></th>
                            <th class="rotate">GQA</th>
                            <th class="rotate">Avg</th>
                            <th class="rotate">SQA<sup>I</sup></th>
                            <th class="rotate">MMMU<sup>V</sup></th>
                            <th class="rotate">MathVista<sup>M</sup></th>
                            <th class="rotate">AI2D</th>
                            <th class="rotate">Avg</th>
                            <th class="rotate">ChartQA</th>
                            <th class="rotate">OCRBench</th>
                            <th class="rotate">TextVQA</th>
                            <th class="rotate">DocVQA</th>
                            <th class="rotate">Avg</th>
                            <th class="rotate">MMVP</th>
                            <th class="rotate">RealworldQA</th>
                            <th class="rotate">CV-Bench<sup>2D</sup></th>
                            <th class="rotate">CV-Bench<sup>3D</sup></th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td>GPT-4V</td>
                            <td>UNK.</td>
                            <td>63.0</td>
                            <td>1409.4</td>
                            <td>75.8</td>
                            <td>69.1</td>
                            <td>36.8</td>
                            <td>65.2</td>
                            <td>75.7</td>
                            <td>56.8</td>
                            <td>49.9</td>
                            <td>78.2</td>
                            <td>77.4</td>
                            <td>78.5</td>
                            <td>64.5</td>
                            <td>78.0</td>
                            <td>88.4</td>
                            <td>62.4</td>
                            <td>50.0</td>
                            <td>61.4</td>
                            <td>64.3</td>
                            <td>73.8</td>
                        </tr>
                        <tr>
                            <td>Gemini-1.0 Pro</td>
                            <td>UNK.</td>
                            <td>-</td>
                            <td>1496.6</td>
                            <td>73.6</td>
                            <td>70.7</td>
                            <td>-</td>
                            <td>-</td>
                            <td>79.5</td>
                            <td>47.9</td>
                            <td>45.2</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>65.9</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>Gemini-1.5 Pro</td>
                            <td>UNK.</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>58.5</td>
                            <td>52.1</td>
                            <td>80.3</td>
                            <td>-</td>
                            <td>81.3</td>
                            <td>-</td>
                            <td>73.5</td>
                            <td>86.5</td>
                            <td>-</td>
                            <td>-</td>
                            <td>67.5</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>Grok-1.5</td>
                            <td>UNK.</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>53.6</td>
                            <td>52.8</td>
                            <td>88.3</td>
                            <td>-</td>
                            <td>76.1</td>
                            <td>-</td>
                            <td>78.1</td>
                            <td>85.6</td>
                            <td>-</td>
                            <td>-</td>
                            <td>68.7</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>MM-1-8B</td>
                            <td>144</td>
                            <td>-</td>
                            <td>1529.3</td>
                            <td>72.3</td>
                            <td>69.9</td>
                            <td>-</td>
                            <td>-</td>
                            <td>72.6</td>
                            <td>37.0</td>
                            <td>35.9</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>MM-1-30B</td>
                            <td>144</td>
                            <td>-</td>
                            <td>1637.6</td>
                            <td>75.1</td>
                            <td>72.1</td>
                            <td>-</td>
                            <td>-</td>
                            <td>81.0</td>
                            <td>44.7</td>
                            <td>39.4</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr class="highlight-gray">
                            <td colspan="22"><i>Base LLM: Llama-3-Ins-8B</i></td>
                        </tr>
                        <tr>
                            <td>Mini-Gemini-HD-8B</td>
                            <td>2880</td>
                            <td>72.7</td>
                            <td><b>1606.0</b></td>
                            <td>72.7</td>
                            <td>73.2</td>
                            <td>64.5</td>
                            <td>55.7</td>
                            <td>75.1</td>
                            <td>37.3</td>
                            <td>37.0</td>
                            <td><b>73.5</b></td>
                            <td>62.9</td>
                            <td>59.1</td>
                            <td>47.7</td>
                            <td>70.2</td>
                            <td>74.6</td>
                            <td>51.5</td>
                            <td>18.7</td>
                            <td>62.1</td>
                            <td>62.2</td>
                            <td>63.0</td>
                        </tr>
                        <tr>
                            <td>LLaVA-NeXT-8B</td>
                            <td>2880</td>
                            <td>72.5</td>
                            <td>1603.7</td>
                            <td>72.1</td>
                            <td>72.7</td>
                            <td><b>65.2</b></td>
                            <td>55.6</td>
                            <td>72.8</td>
                            <td>41.7</td>
                            <td>36.3</td>
                            <td>71.6</td>
                            <td>63.9</td>
                            <td>69.5</td>
                            <td>49.0</td>
                            <td>64.6</td>
                            <td>72.6</td>
                            <td>56.6</td>
                            <td>38.7</td>
                            <td>60.1</td>
                            <td>62.2</td>
                            <td>65.3</td>
                        </tr>
                        <tr class="highlight-orange">
                            <td>Cambrian-1-8B</td>
                            <td class="highlight-blue">576</td>
                            <td><b>73.1</b></td>
                            <td>1,547.1</td>
                            <td><b>75.9</b></td>
                            <td><b>74.7</b></td>
                            <td>64.6</td>
                            <td><b>61.3</b></td>
                            <td><b>80.4</b></td>
                            <td><b>42.7</b></td>
                            <td><b>49.0</b></td>
                            <td>73.0</td>
                            <td><b>71.3</b></td>
                            <td><b>73.3</b></td>
                            <td><b>62.4</b></td>
                            <td><b>71.7</b></td>
                            <td><b>77.8</b></td>
                            <td><b>65.0</b></td>
                            <td><b>51.3</b></td>
                            <td><b>64.2</b></td>
                            <td><b>72.3</b></td>
                            <td><b>72.0</b></td>
                        </tr>
                        <tr class="highlight-gray">
                            <td colspan="22"><i>Base LLM: Vicuna-1.5-13B</i></td>
                        </tr>
                        <tr>
                            <td>Mini-Gemini-HD-13B</td>
                            <td>2880</td>
                            <td>70.7</td>
                            <td>1597.0</td>
                            <td>68.6</td>
                            <td>70.6</td>
                            <td>63.7</td>
                            <td>54.1</td>
                            <td>71.9</td>
                            <td>37.3</td>
                            <td>37.0</td>
                            <td>70.1</td>
                            <td>60.8</td>
                            <td>56.6</td>
                            <td>46.6</td>
                            <td>70.2</td>
                            <td>69.8</td>
                            <td>49.4</td>
                            <td>19.3</td>
                            <td>57.5</td>
                            <td>53.6</td>
                            <td>67.3</td>
                        </tr>
                        <tr>
                            <td>LLaVA-NeXT-13B</td>
                            <td>2880</td>
                            <td>69.9</td>
                            <td>1575.0</td>
                            <td>70.0</td>
                            <td>65.6</td>
                            <td><b>65.4</b></td>
                            <td>53.7</td>
                            <td>73.5</td>
                            <td>36.2</td>
                            <td>35.1</td>
                            <td>70.0</td>
                            <td>62.9</td>
                            <td>62.2</td>
                            <td>51.4</td>
                            <td>67.1</td>
                            <td>70.9</td>
                            <td>55.9</td>
                            <td>36.0</td>
                            <td>59.1</td>
                            <td>62.7</td>
                            <td>65.7</td>
                        </tr>
                        <tr class="highlight-orange">
                            <td>Cambrian-1-13B</td>
                            <td class="highlight-blue">576</td>
                            <td><b>73.7</b></td>
                            <td><b>1,610.4</b></td>
                            <td><b>75.7</b></td>
                            <td><b>74.4</b></td>
                            <td>64.3</td>
                            <td><b>60.2</b></td>
                            <td><b>79.3</b></td>
                            <td><b>40.0</b></td>
                            <td><b>48.0</b></td>
                            <td><b>73.6</b></td>
                            <td><b>71.3</b></td>
                            <td><b>73.8</b></td>
                            <td><b>61.9</b></td>
                            <td><b>72.8</b></td>
                            <td><b>76.8</b></td>
                            <td><b>62.2</b></td>
                            <td><b>41.3</b></td>
                            <td><b>63.0</b></td>
                            <td><b>72.5</b></td>
                            <td><b>71.8</b></td>
                        </tr>
                        <tr class="highlight-gray">
                            <td colspan="22"><i>Base LLM: Hermes2-Yi-34B</i></td>
                        </tr>
                        <tr>
                            <td>Mini-Gemini-HD-34B</td>
                            <td>2880</td>
                            <td>76.2</td>
                            <td>1659.0</td>
                            <td>80.6</td>
                            <td>75.3</td>
                            <td>65.8</td>
                            <td>62.4</td>
                            <td>77.7</td>
                            <td>48.0</td>
                            <td>43.4</td>
                            <td><b>80.5</b></td>
                            <td>68.1</td>
                            <td>67.6</td>
                            <td>51.8</td>
                            <td>74.1</td>
                            <td><b>78.9</b></td>
                            <td>63.8</td>
                            <td>37.3</td>
                            <td>67.2</td>
                            <td>71.5</td>
                            <td>79.2</td>
                        </tr>
                        <tr>
                            <td>LLaVA-NeXT-34B</td>
                            <td>2880</td>
                            <td>76.0</td>
                            <td>1633.2</td>
                            <td>79.3</td>
                            <td><b>75.9</b></td>
                            <td><b>67.1</b></td>
                            <td>62.5</td>
                            <td>81.8</td>
                            <td>46.7</td>
                            <td>46.5</td>
                            <td>74.9</td>
                            <td>67.7</td>
                            <td>68.7</td>
                            <td>54.5</td>
                            <td>69.5</td>
                            <td>78.1</td>
                            <td>64.0</td>
                            <td>47.3</td>
                            <td>61.0</td>
                            <td>73.0</td>
                            <td>74.8</td>
                        </tr>
                        <tr class="highlight-orange">
                            <td>Cambrian-1-34B</td>
                            <td class="highlight-blue">576</td>
                            <td><b>76.8</b></td>
                            <td><b>1689.3</b></td>
                            <td><b>81.4</b></td>
                            <td>75.3</td>
                            <td>65.8</td>
                            <td><b>67.0</b></td>
                            <td><b>85.6</b></td>
                            <td><b>49.7</b></td>
                            <td><b>53.2</b></td>
                            <td>79.7</td>
                            <td><b>71.9</b></td>
                            <td><b>75.6</b></td>
                            <td><b>60.0</b></td>
                            <td><b>76.7</b></td>
                            <td>75.5</td>
                            <td><b>68.5</b></td>
                            <td><b>52.7</b></td>
                            <td><b>67.8</b></td>
                            <td><b>74.0</b></td>
                            <td><b>79.7</b></td>
                        </tr>
                        </tbody>
                    </table>
                </div>
                <figcaption style="text-align: center; width: 140%;">
                    Table 5: Cambrian-1 outperforms other open-source models and achieves comparable performance with proprietary models, while using only 576 visual tokens.
                </figcaption>
            </div>
            <p style="padding: 1em"></p>
            <d-figure id="fig-comparison">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/comparison.PNG" alt="Cambrian-1 outperforms other open-source models and achieves comparable performance with proprietary models">
                    <figcaption>
                        <strong>Figure 12:</strong> Cambrian-1 outperforms other open-source models and achieves comparable performance with proprietary models.
                    </figcaption>
                </figure>
            </d-figure>

        </div>

        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                In this paper, we introduce <span class="small-caps">VDC</span>, a novel video detailed captioning benchmark designed to evaluate comprehensive and coherent textual descriptions of video content. 
                We also propose <span class="small-caps">VDCScore</span> for better evaluating. Besides, by leveraging the token merging strategy, we significantly reduce the computational overhead without compromising performance. 
                Our extensive evaluation on various video and image captioning benchmarks demonstrated that <span class="small-caps">AuroraCap</span> achieves competitive results, even outperforming state-of-the-art models in some tasks. 
                We also conduct thorough ablation studies to validate the effectiveness of token merging and other aspects of our model. 
                We found that the current model performs poorly in terms of the trade-off between performance and the scale of input tokens. 
                Additionally, there is still room for improvement in camera handling and detailed captioning. We hope that <span class="small-caps">VDC</span> can bring new insights to the video detailed captioning task.
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{tong2024cambrian,<br>
                &nbsp;&nbsp;title={{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}},<br>
                &nbsp;&nbsp;author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng, and Iyer, Adithya and Pan, Xichen and Wang, Austin and Fergus, Rob and LeCun, Yann and Xie, Saining},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2406.16860},<br>
                &nbsp;&nbsp;year={2024}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>   
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
    </body>
</html>
