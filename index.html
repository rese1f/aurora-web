<!doctype html>
<html lang="en">
    <head>
        <title>AuroraCap</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="assets/js/template.v2.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="assets/js/cross_fade.js"></script>
        <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.35.2/gradio.js"></script>
        <link rel="stylesheet" href="assets/css/style.css">
        <!-- <link rel="icon" type="favicon/png" href="favicon.png"> -->
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <style>
            /* Optional styling to ensure that mathematical symbols are properly formatted */
            .math {
                font-family: 'Times New Roman', Times, serif;
                font-style: italic;
            }
            .highlight-red {
                background-color: #b83a4b36;
            }
        </style>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px">AuroraCap</h1>
                    <h2>Less is More for Efficient Video Detailed Captioning and a New Benchmark</h2>
                        <p>
                            AuroraCap is a multimodal LLM designed for image and video detailed captioning. We also release VDC, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets.
                        </p>
                    

                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <!-- <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>PDF</span>
                        </a> -->
                        <!-- replace image -->
                        <a href="https://github.com/rese1f/aurora" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>GitHub</span>
                        </a>
                        <!-- <br> -->
                        <a href="https://huggingface.co/collections/Reself/auroracap-66d117ffe13bedda96702013" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Model</span>
                        </a>
                        <a href="https://huggingface.co/datasets/Reself/AuroraCap-trainset" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Data</span>
                        </a>                        
                        <a href="https://huggingface.co/datasets/Reself/Video-Detailed-Caption" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>VDC Benchmark</span>
                        </a>
                    </div>
                </div>
                <div class="header-content">
                    <div class="icon-item">
                        <img src="./assets/img/icons/obs.svg" alt="Observation Icon">
                        <div><strong>Observation</strong>: We found that image-based LLaVA-like multimodal large language models can be easily adapted to a video one without any additional parameters but only with high-quality video-text instruction data for finetuning.</div>
                    </div>
                    <br>
                    <div class="icon-item">
                        <img src="./assets/img/icons/eff.svg" alt="Efficency Icon">
                        <div><strong>Efficency</strong>: We can reduce the number of token used for image or video before injecting into LLM with marginal performance drop. Therefore, we propose AuroraCap, which is the state-of-the-art open-sourced video captioning model.</div>
                    </div>
                    <br>
                    <div class="icon-item">
                        <img src="./assets/img/icons/benchmark.svg" alt="Benchmark Icon">
                        <div><strong>Benchmark</strong>: We also release VDC, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets.</div>
                    </div>
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://rese1f.github.io/" class="author-link" target="_blank">Wenhao Chai</a><sup> 1,2</sup> &emsp;
                    <a href="https://espere-1119-song.github.io/" class="author-link" target="_blank">Enxin Song</a> &emsp;
                    <a href="https://yilundu.github.io/" class="author-link" target="_blank">Yilun Du</a><sup> 4,5</sup> &emsp;
                    <a href="https://cs.stanford.edu/~chenlin/" class="author-link" target="_blank">Chenlin Meng</a><sup> 2,3</sup> &emsp;
                    <a href="https://scholar.google.com/citations?user=WjF1dugAAAAJ" class="author-link" target="_blank">Vashisht Madhavan</a><sup> 2</sup> &emsp;
                    <a href="https://omerbt.github.io/" class="author-link" target="_blank">Omer Bar-Tal</a><sup> 2</sup> &emsp;
                    <a href="https://people.ece.uw.edu/hwang/" class="author-link" target="_blank">Jeng-Neng Hwang</a><sup> 1</sup> &emsp;
                    <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie</a><sup> 6</sup> &emsp;
                    <a href="https://nlp.stanford.edu/~manning/" class="author-link" target="_blank">Christopher D. Manning</a><sup> 3</sup> &emsp;
                    </p>
                <p>
                    <sup>1</sup> <a href="https://www.washington.edu/" class="affiliation-link" id="affiliation" target="_blank">University of Washington</a> &emsp;
                    <sup>2</sup> <a href="https://pika.art" class="affiliation-link" id="affiliation" target="_blank">Pika Lab</a> &emsp;
                    <sup>3</sup> <a href="https://www.stanford.edu/" class="affiliation-link" id="affiliation" target="_blank">Stanford University</a> &emsp;
                    <br>
                    <sup>4</sup> <a href="https://www.mit.edu/" class="affiliation-link" id="affiliation" target="_blank">Massachusetts Institute of Technology</a> &emsp;
                    <sup>5</sup> <a href="https://www.harvard.edu/" class="affiliation-link" id="affiliation" target="_blank">Harvard University</a> &emsp;
                    <br>
                    <sup>6</sup> <a href="https://www.nyu.edu/" class="affiliation-link" id="affiliation" target="_blank">New York University</a>
                </p>
            </div>
        </div>
        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#auroracap">AuroraCap</a></div>
                <div><a href="#vdc">VDC</a></div>
                <div><a href="#evaluation">Evaluation</a></div>
                <div><a href="#ablation">Ablation</a></div>
                <div><a href="#case">Case Study</a></div>
            </nav>
        </d-contents>
        
        <p>
            We propose AuroraCap, a simple video captioner based on multimodal large language model. 
            We follow the simplest architecture design without additional parameters for temporal modeling. 
            To address the overhead caused by lengthy video sequences, we implement the token merging strategy, reducing the number of visual tokens input. 
            We present VDC, a video detailed captioning benchmark with over one thousand carefully annotated structured captions. 
            In addition, we propose a new LLM-assisted metric VDCScore for bettering evaluation. 
            We adopt a divide-and-conquer strategy to transform the evaluation of long captions into multiple short question-answering pairs. 
        </p>
        <section id="auroracap">
            <h2>AuroraCap: A Efficient Video Detailed Captioner</h2>
            <p>
                <h3 class="text">Architecture</h1>
                <p>
                    <strong>LLaVA.<d-cite key="liu2024visual"></d-cite></strong> 
                    To effectively leverage the capabilities of both the pre-trained LLM and visual model, LLaVA adapt a simple multilayer perceptron (MLP) projection layer to connect each patch tokens of image features into the word embedding space. 
                </p>
                <p class="text">
                    <strong>Token merging.<d-cite key="bolya2022token"></d-cite></strong> 
                    To increase the throughput of existing ViT models, Token Merging is proposed to gradually combines similar tokens in a transformer to reduce the number of tokens passing through ViT models. 
                    Token Merging has been proven to be effective on image and video classification tasks even without the need for training.
                    We conduct frame-wise token merging in AuroraCap, where the feature is extracted by CLIP ViT-H model. 
                    We show token merging visualization examples from  COCO, VG, SA-1B as follows:
                    <d-figure>
                        <figure class="l-body">
                            <div id='tomevis_div'></div>
                            <script src="https://d3js.org/d3.v7.min.js"></script>
                            <script src="assets/js/tome_vis.js"></script>
                            <figcaption>
                                Token merging visualization. From left to right, the number of visual tokens representing the images are 490, 154, 18, and 6.
                            </figcaption>
                        </figure>
                    </d-figure>
                </p>
            </p>
            <p>
                <h3 class="text">Training Recipe</h3>
                <p class="text">
                    We use over 20 million high-quality image/video-text pairs to train AuroraCap in three stages. The training datasets are released at <a href="https://huggingface.co/datasets/Reself/AuroraCap-trainset" target="_blank">HuggingFace</a>.
                </p>
                <p class="text">
                    <strong>Pretraining stage.</strong> 
                    We first align visual features with the word embedding space of LLMs. 
                    To achieve this, we freeze the pretrained ViT and LLM, training solely the vision-language connector. 
                </p>
                <p class="text">
                    <strong>Vision stage.</strong> 
                    We unfreeze the pretrained ViT while freezing the LLM during vision stage and train with the public data among various computer vision tasks to get better generalization.
                </p>
                <p class="text">
                    <strong>Language stage.</strong> 
                    Finally, we conduct end-to-end training, which means all the components are trainable, with the most high-quality public data during language stage. 
                </p>
            </p>

        </section>
        <section id="vdc">
            <h2>VDC: A New Video Detailed Captioning Benchmark</h2>

            <div style="display: flex; flex-direction: column; align-items: center;">
                <div class="table-container">
                    <table class="data-table">
                        <thead>
                        <tr>
                            <th class="tb-hdr">Dataset</th>
                            <th>Theme</th>
                            <th># Video</th>
                            <th># Clip</th>
                            <th># Caption</th>
                            <th># Word</th>
                            <th># Vocab.</th>
                            <th>Ave. Length</th>
                        </tr>
                        <tr>
                            <td class="italic">MSVD</td>
                            <td>Open</td>
                            <td>1,970</td>
                            <td>1,970</td>
                            <td>70,028</td>
                            <td>607,339</td>
                            <td>13,010</td>
                            <td>8.67</td>
                        </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="italic">MSR-VTT</td>
                                <td>Open</td>
                                <td>7,180</td>
                                <td>10,000</td>
                                <td>200,000</td>
                                <td>1,856,523</td>
                                <td>29,316</td>
                                <td>9.28</td>
                            </tr>
                            <tr>
                                <td class="italic">ActivityNet</td>
                                <td>Open</td>
                                <td>20,000</td>
                                <td>100,000</td>
                                <td>100,000</td>
                                <td>1,340,000</td>
                                <td>15,564</td>
                                <td>13.40</td>
                            </tr>
                            <tr>
                                <td class="italic">S-MiT</td>
                                <td>Open</td>
                                <td>515,912</td>
                                <td>515,912</td>
                                <td>515,912</td>
                                <td>5,618,064</td>
                                <td>50,570</td>
                                <td>10.89</td>
                            </tr>
                            <tr>
                                <td class="italic">M-VAD</td>
                                <td>Movie</td>
                                <td>92</td>
                                <td>48,986</td>
                                <td>55,905</td>
                                <td>519,933</td>
                                <td>18,269</td>
                                <td>9.30</td>
                            </tr>
                            <tr>
                                <td class="italic">MPII-MD</td>
                                <td>Movie</td>
                                <td>94</td>
                                <td>68,337</td>
                                <td>68,375</td>
                                <td>653,467</td>
                                <td>24,549</td>
                                <td>9.56</td>
                            </tr>
                            <tr>
                                <td class="italic">Youcook2</td>
                                <td>Cooking</td>
                                <td>2,000</td>
                                <td>15,400</td>
                                <td>15,400</td>
                                <td>121,418</td>
                                <td>2,583</td>
                                <td>7.88</td>
                            </tr>
                            <tr>
                                <td class="italic">Charades</td>
                                <td>Human</td>
                                <td>9,848</td>
                                <td>10,000</td>
                                <td>27,380</td>
                                <td>607,339</td>
                                <td>13,000</td>
                                <td>22.18</td>
                            </tr>
                            <tr>
                                <td class="italic">VATEX</td>
                                <td>Open</td>
                                <td>41,300</td>
                                <td>41,300</td>
                                <td>413,000</td>
                                <td>4,994,768</td>
                                <td>44,103</td>
                                <td>12.09</td>
                            </tr>
                            <tr>
                                <td class="italic">VDC (ours)</td>
                                <td>Open</td>
                                <td>1,027</td>
                                <td>1,027</td>
                                <td>1,027</td>
                                <td>515,441</td>
                                <td>20,419</td>
                                <td class="highlight-red">500.91</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <figcaption style="text-align: center; width: 140%;">
                    Benchmark comparison for video captioning task. Ave. Length indicates the average number of words per caption. 
                </figcaption>
            </div>

            <p class="text">
                <h3 class="text">Benchmark Collection and Processing</h3>
                <strong>Video collection and processing.</strong>
                    We building VDC upon Panda-70M, Ego4D, Mixkit, Pixabay, and Pexels. 
                    We first split the video into clips and apply dense frame extraction, then manually replacing blurry frames with adjacent clear ones.
                    <br>
                <strong>Structured detailed captions construction pipeline.</strong>
                    We develop a structured detailed captions construction pipeline to generate extra detailed descriptions from various perspectives, significantly extending the length and enhancing the richness compared to previous benchmarks. 
                    The structured detailed captions includes the following categories:
                    <br>
                    <br>
                    <ol class="text" style="margin-left: 20px;">
                        <li style="margin-bottom: 3px;"><strong>Camera caption.</strong>
                            Describe the camera work in detail, including shot types, angles, movements, transitions, and any special effects used to enhance the video.
                        </li>
                        <li style="margin-bottom: 3px;"><strong>Short caption.</strong>
                            Summarize the video in one detailed sentence, capturing key actions and the overall mood.
                        </li>
                        <li style="margin-bottom: 3px;"><strong>Background caption.</strong>
                            Provide a detailed description of the background, including objects, location, weather, time, and any dynamic elements.
                        </li>
                        <li style="margin-bottom: 3px;"><strong>Main Object caption.</strong>
                            Give a thorough description of the main subject's actions, attributes, interactions, and movements throughout the video frames.
                        </li>
                        <li style="margin-bottom: 3px;"><strong>Detailed caption.</strong>
                            Generate a detailed, vivid caption for the video, covering all categories, ensuring it's engaging, informative, and rich enough for AI to recreate the video content.
                        </li>
                    </ol>
                    <br>
                    To generate detailed, fine-grained, and accurate captions, we leverage GPT-4o to produce video descriptions. 
                    We design a hierarchical prompt strategy to efficiently obtain accurate structured captions and detailed captions in two conversation rounds: (1) Structured Captions Generation and (2) Detailed Captions Integration. 
                    <br>
                 
                <d-figure id="fig-vision_connector">
                    <figure>
                        <img data-zoomable="" draggable="false" src="assets/img/vdc.png" alt="Video length in VDC" style="width: 100%;display: block; margin: 0 auto;">
                        <figcaption>
                            Distribution of the video length and structured caption length in VDC.
                        </figcaption>
                    </figure>
                </d-figure>

            </p>
            <p class="text">
                <h3 class="text">Evaluation Metric Design and Leaderboard</h1>
                <strong>VDCScore: Evaluating Detailed Captions with LLMs</strong>
                    <br>
                    We introduce VDCScore, a novel quantitative metric that utilizes LLMs to evaluate the similarity between predicted and ground-truth detailed captions through a divide-and-conquer approach.
                    The core idea of VDCScore is to decompose long detailed captions into multiple short question-answering pairs, avergae the evaluation of each pair as the final result.
                    <d-figure id="fig-vision_connector">
                        <figure>
                            <img data-zoomable="" draggable="false" src="assets/img/vdcscore.png" alt="Video length in VDC" style="width: 100%;display: block; margin: 0 auto;">
                            <figcaption>
                                VDCScore evaluation pipeline.
                            </figcaption>
                        </figure>
                    </d-figure>
            </p>

            <p class="text"> 
                <h3 class="text">Benchmark Examples</h3>
                <p class="text">
                    We present several data examples from VDC dataset in following Figure.
                </p>
                
            </p>




        </section>

        
        <section id="evaluation">
            <p>
                <h2 class="text">Evaluation</h1>
                <p class="text">
                    <strong>Benchmarking video detailed captioning.</strong>
                    AuroraCap achieves superior performance in video detailed captioning while utilizing significantly fewer visual tokens than other models, fully highlighting the efficiency of AuroraCap. 
                    <d-figure id="fig-vision_connector">
                        <figure>
                            <img data-zoomable="" draggable="false" src="assets/img/vdc_benchmaark.png" alt="Video length in VDC" style="width: 100%;display: block; margin: 0 auto;">
                            <figcaption>
                                Comparison between various models with different number of visual tokens input on VDC.
                            </figcaption>
                        </figure>
                    </d-figure>
                    We present a quantitative comparison between AuroraCap with existing state-of-the-art multimodal LLMs across various sections of structured captions in VDC. 
                    <br>
                    <br>
                    <div style="display: flex; flex-direction: column; align-items: center;">
                        <div class="table-container">
                            <table class="data-table">
                                <thead>
                                <tr>
                                    <th class="tb-hdr">Model</th>
                                    <th class="tb-hdr">Camera</th>
                                    <th class="tb-hdr">Short</th>
                                    <th class="tb-hdr">Background</th>
                                    <th class="tb-hdr">Main Object</th>
                                    <th class="tb-hdr">Detailed</th>
                                </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td class="italic">Vicuna-v1.5-7B</td>
                                        <td>21.68</td>
                                        <td>23.06</td>
                                        <td>22.02</td>
                                        <td>22.64</td>
                                        <td>23.09</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Llama-3.1-8B</td>
                                        <td>17.83</td>
                                        <td>17.90</td>
                                        <td>19.52</td>
                                        <td>19.57</td>
                                        <td>20.10</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Gemini-1.5 Pro</td>
                                        <td>38.68</td>
                                        <td>35.71</td>
                                        <td>43.84</td>
                                        <td>47.32</td>
                                        <td>43.11</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLAMA-VID</td>
                                        <td>39.47</td>
                                        <td>29.92</td>
                                        <td>28.01</td>
                                        <td>31.24</td>
                                        <td>25.67</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Video-ChatGPT-7B</td>
                                        <td>37.46</td>
                                        <td>29.36</td>
                                        <td>33.68</td>
                                        <td>30.47</td>
                                        <td>24.61</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">VILA-7B</td>
                                        <td>34.33</td>
                                        <td>30.40</td>
                                        <td>35.15</td>
                                        <td>33.38</td>
                                        <td>29.78</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Video-LLAVA-7B</td>
                                        <td>37.48</td>
                                        <td>30.67</td>
                                        <td>32.50</td>
                                        <td>36.01</td>
                                        <td>27.36</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLAVA-1.5-7B</td>
                                        <td>38.38</td>
                                        <td>28.61</td>
                                        <td>34.86</td>
                                        <td>34.62</td>
                                        <td>33.43</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LongVA-7B</td>
                                        <td>35.32</td>
                                        <td>31.94</td>
                                        <td>36.39</td>
                                        <td>40.95</td>
                                        <td>27.91</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLAVA-1.5-13B</td>
                                        <td>38.97</td>
                                        <td>30.89</td>
                                        <td>34.79</td>
                                        <td>36.27</td>
                                        <td>33.00</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLAVA-NeXT-V7B</td>
                                        <td>39.73</td>
                                        <td>30.63</td>
                                        <td>36.54</td>
                                        <td>36.54</td>
                                        <td>33.84</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLAVA-1.6-7B</td>
                                        <td>36.50</td>
                                        <td>31.91</td>
                                        <td>37.58</td>
                                        <td>36.03</td>
                                        <td>36.47</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLAVA-1.6-13B</td>
                                        <td>35.61</td>
                                        <td>31.90</td>
                                        <td class="highlight-red">38.90</td>
                                        <td>36.65</td>
                                        <td>36.18</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">ShareGPT4Video-8B</td>
                                        <td>33.28</td>
                                        <td class="highlight-red">39.05</td>
                                        <td>35.77</td>
                                        <td>37.12</td>
                                        <td>35.62</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLAVA-OV-7B</td>
                                        <td>37.82</td>
                                        <td>32.58</td>
                                        <td>37.43</td>
                                        <td>38.21</td>
                                        <td>41.20</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">InternVL-2-8B</td>
                                        <td>39.08</td>
                                        <td>33.02</td>
                                        <td>37.47</td>
                                        <td class="highlight-red">44.16</td>
                                        <td>34.89</td>
                                    </tr>
                                    <tr>
                                        <td><strong>AuroraCap-7B</strong></td>
                                        <td class="highlight-red">43.50</td>
                                        <td>32.07</td>
                                        <td>35.92</td>
                                        <td>39.02</td>
                                        <td class="highlight-red">41.30</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <figcaption style="text-align: center; width: 140%;">
                            Comparison of AuroraCap with LLM-based baseline methods on VDCScore under zero-shot structured captions setting. 
                        </figcaption>
                    </div>
                    <br>
                    <strong>Image captioning.</strong> 
                    We evaluate AuroraCap using CIDEr, BELU-4, BELU-1, METEOR, and ROUGE-L metric on Flickr, NoCaps, and COCO-Cap benchmarks and compare it with LLM-based state-of-the-art methods. 
                    AuroraCap shows good performance under zero-shot settings. 
                    Notice that these benchmarks all contain short captions consisting of a single sentence, so they only partially reflect the model's performance.
                    <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                        <div class="table-container">
                            <table class="data-table">
                                <thead>
                                <tr>
                                    <th class="tb-hdr" rowspan="2">Model</th>
                                    <th colspan="2" class="tb-hdr">Flickr (31,784)</th>
                                    <th colspan="2" class="tb-hdr">NoCaps (4,500)</th>
                                    <th colspan="2" class="tb-hdr">COCO-Cap (5,000)</th>
                                </tr>
                                <tr>
                                    <th>C</th>
                                    <th>R</th>
                                    <th>C</th>
                                    <th>R</th>
                                    <th>C</th>
                                    <th>R</th>
                                </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td class="italic">LLaVA-1.5-7B</td>
                                        <td>74.9</td>
                                        <td>52.8</td>
                                        <td>105.5</td>
                                        <td>59.4</td>
                                        <td>110.3</td>
                                        <td>55.5</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLaVA-1.5-13B</td>
                                        <td>79.4</td>
                                        <td>53.9</td>
                                        <td>109.2</td>
                                        <td>60.3</td>
                                        <td>115.6</td>
                                        <td>56.5</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLaVA-1.6-7B</td>
                                        <td>68.4</td>
                                        <td>50.3</td>
                                        <td>88.4</td>
                                        <td>54.6</td>
                                        <td>99.9</td>
                                        <td>52.4</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLaVA-1.6-13B</td>
                                        <td>66.6</td>
                                        <td>48.8</td>
                                        <td>88.1</td>
                                        <td>54.9</td>
                                        <td>101.8</td>
                                        <td>52.1</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">MiniCPM-V-3B</td>
                                        <td>66.8</td>
                                        <td>51.0</td>
                                        <td>89.9</td>
                                        <td>55.8</td>
                                        <td>94.2</td>
                                        <td>52.3</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">DeCap</td>
                                        <td>56.7</td>
                                        <td>—</td>
                                        <td>42.7</td>
                                        <td>—</td>
                                        <td>91.2</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Flamingo-80B</td>
                                        <td>67.2</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>84.3</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Chameleon-34B</td>
                                        <td>74.7<sup>2</sup></td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>120.2<sup>2</sup></td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">GPT-4V</td>
                                        <td>55.3<sup>8</sup></td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>78.5<sup>8</sup></td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Gemini-1.5 Pro</td>
                                        <td>82.2<sup>4</sup></td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>99.8<sup>2</sup></td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td><strong>AuroraCap-7B</strong></td>
                                        <td class="highlight-red">88.9</td>
                                        <td class="highlight-red">55.4</td>
                                        <td class="highlight-red">111.4</td>
                                        <td class="highlight-red">60.6</td>
                                        <td class="highlight-red">120.8</td>
                                        <td class="highlight-red">57.2</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <figcaption style="text-align: center; width: 140%;">
                            Comparison AuroraCap with SoTA methods on image captioning benchmarks under zero-shot setting.
                        </figcaption>
                    </div>
                </p>
                <br>
                <p class="text">
                    <strong>Video captioning.</strong> 
                    Although the current video captioning benchmarks are only contains one-sentence captions, to compare with prior work, we similarly evaluate on these benchmarks. 
                    We evaluate AuroraCap on MSR-VTT, VATEX, and ActivityNet Captions and compare it with other methods.
                    <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                        <div class="table-container">
                            <table class="data-table">
                                <thead>
                                <tr>
                                    <th class="tb-hdr" rowspan="2">Model</th>
                                    <th colspan="5" class="tb-hdr">MSR-VTT (1,000)</th>
                                    <th colspan="5" class="tb-hdr">VATEX (1,000)</th>
                                </tr>
                                <tr>
                                    <th>C</th>
                                    <th>B@1</th>
                                    <th>B@4</th>
                                    <th>M</th>
                                    <th>R</th>
                                    <th>C</th>
                                    <th>B@1</th>
                                    <th>B@4</th>
                                    <th>M</th>
                                    <th>R</th>
                                </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>ZeroCap</td>
                                        <td>9.6</td>
                                        <td>—</td>
                                        <td>2.9</td>
                                        <td>16.3</td>
                                        <td>35.4</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td>DeCap</td>
                                        <td>18.6</td>
                                        <td>—</td>
                                        <td>14.7</td>
                                        <td>20.4</td>
                                        <td>—</td>
                                        <td>18.7</td>
                                        <td>—</td>
                                        <td>13.1</td>
                                        <td>15.3</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td>PaLI-3</td>
                                        <td>21.3</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td>Ma et al.</td>
                                        <td>22.1</td>
                                        <td>—</td>
                                        <td>3.5</td>
                                        <td>17.3</td>
                                        <td>28.7</td>
                                        <td>23.9</td>
                                        <td>—</td>
                                        <td>2.8</td>
                                        <td>14.1</td>
                                        <td>23.5</td>
                                    </tr>
                                    <tr>
                                        <td>LLaVA-7B</td>
                                        <td>16.9</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td>Video-LLAMA</td>
                                        <td>2.3</td>
                                        <td>—</td>
                                        <td>4.9</td>
                                        <td>16.8</td>
                                        <td>—</td>
                                        <td>3.8</td>
                                        <td>—</td>
                                        <td>4.3</td>
                                        <td>16.3</td>
                                        <td>21.8</td>
                                    </tr>
                                    <tr>
                                        <td>AuroraCap-7B</td>
                                        <td class="highlight-red">33.1</td>
                                        <td class="highlight-red">58.6</td>
                                        <td class="highlight-red">21.0</td>
                                        <td class="highlight-red">23.9</td>
                                        <td class="highlight-red">49.5</td>
                                        <td class="highlight-red">33.8</td>
                                        <td class="highlight-red">57.1</td>
                                        <td class="highlight-red">18.4</td>
                                        <td class="highlight-red">19.0</td>
                                        <td class="highlight-red">40.8</td>
                                    </tr>

                                </tbody>
                            </table>
                        </div>
                        <figcaption style="text-align: center; width: 140%;">
                            Comparison AuroraCap with SoTA methods on existing video captioning benchmarks under zero-shot setting.
                        </figcaption>
                    </div>

                </p>
            </p>

        </section>


        <section id="ablation">
            <h2>Ablation Study on Token Merging Ratio</h2>
                    As a core training and inference strategy of AuroraCap, token merging plays a significant role in reducing the number of visual tokens. 
                    We further study how the video detailed captioning capability is influenced by token merge ratio. 
                    We define the performance percentage as the proportion between the highest and lowest values on the entire performance curve. 
                    We highlight the token merging ratio when achieving 90% and 80% performance with the dash line and filled area.
                    We found that token merging significantly reduces the number of tokens while maintaining minimal performance drop, and even showing improvement in some tasks. 
                    <d-figure>
                        <figure class="l-body">
                            <div id='tome_div'></div>
                            <script src="https://d3js.org/d3.v7.min.js"></script>
                            <script src="assets/js/tome_curve.js"></script>
                            <figcaption>
                                Ablation study on token merging ratio on various image and video understanding tasks.
                            </figcaption>
                        </figure>
                    </d-figure>

                    To assess the inference speed, we utilize the inference time per video question-answering pair in seconds (TPV) as an evaluative metric. 
                    Figure below indicates the minimum TPV achievable in our settings including with or without token merging and SGLang across seven video understanding datasets. 
                    Reducing the visual tokens and using SGLang result in excellent inference times per video question-answering pair while all the datasets with short video and question inputs. 
                    <d-figure id="fig-vision_connector">
                        <figure>
                            <img data-zoomable="" draggable="false" src="assets/img/sglang.png" alt="Video length in VDC" style="width: 100%;display: block; margin: 0 auto;">
                            <figcaption>
                                Comparison between different inference settings: 
                                <b>A</b>: R<sub>vtk</sub> = 1.0</span>, without SGLang, 
                                <b>B</b>: R<sub>vtk</sub> = 0.1</span>, without SGLang, 
                                <b>C</b>: R<sub>vtk</sub> = 1.0</span>, with SGLang, 
                                <b>D</b>: R<sub>vtk</sub> = 0.1</span>, with SGLang. 
                                The number indicates the maximum inference time in seconds for each benchmark.
                            </figcaption>
                        </figure>
                    </d-figure>
        </section>

        <section id="case">
            <h2>Case Study</h2>
            <p class="text">
                We perform an extensive case study of AuroraCap on a variety of videos for video detailed captioning. 
                As shown as followings, AuroraCap is capable of providing excellent detailed captions regarding the camera motion, background and main object with less hallucination. 
            </p>

            <div id="cases_div">
                <div id="video_overlay">
                    <script src="assets/js/cases.js"></script>
                </div>
            </div>
            
        </section>

        
        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{auroracap,<br>
                &nbsp;&nbsp;title={},<br>
                &nbsp;&nbsp;author={},<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:},<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
          </d-appendix>
          
            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>

          <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>

        <!-- <script type="text/bibliography">
        
        </script> -->
        <script src="assets/js/contents_bar.js"></script>
        
        

    </body>
</html>