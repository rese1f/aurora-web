<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="template.v2.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.35.2/gradio.js"></script>
        <link rel="stylesheet" href="style.css">
        <!-- <link rel="icon" type="favicon/png" href="favicon.png"> -->
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px">AuroraCap</h1>
                    <h2>Less is More for Efficient Video Detailed Captioning and a New Benchmark</h2>
                        <p>
                            AuroraCap is a multimodal LLM designed for image and video detailed captioning. We also release VDC, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets.
                        </p>
                    

                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <!-- <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>PDF</span>
                        </a> -->
                        <!-- replace image -->
                        <a href="https://github.com/rese1f/aurora" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>GitHub</span>
                        </a>
                        <!-- <br> -->
                        <a href="https://huggingface.co/collections/Reself/auroracap-66d117ffe13bedda96702013" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Model</span>
                        </a>
                        <a href="https://huggingface.co/datasets/Reself/AuroraCap-trainset" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Data</span>
                        </a>                        
                        <a href="https://huggingface.co/datasets/Reself/Video-Detailed-Caption" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>VDC Benchmark</span>
                        </a>
                    </div>
                </div>
                <div class="header-content">
                    <div class="icon-item">
                        <img src="./static/img/icons/obs.svg" alt="Observation Icon">
                        <div><strong>Observation</strong>: We found that image-based LLaVA-like multimodal large language models can be easily adapted to a video one without any additional parameters but only with high-quality video-text instruction data for finetuning.</div>
                    </div>
                    <br>
                    <div class="icon-item">
                        <img src="./static/img/icons/eff.svg" alt="Efficency Icon">
                        <div><strong>Efficency</strong>: We can reduce the number of token used for image or video before injecting into LLM with marginal performance drop. Therefore, we propose AuroraCap, which is the state-of-the-art open-sourced video captioning model.</div>
                    </div>
                    <br>
                    <div class="icon-item">
                        <img src="./static/img/icons/benchmark.svg" alt="Benchmark Icon">
                        <div><strong>Benchmark</strong>: We also release VDC, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions than existing datasets.</div>
                    </div>
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://rese1f.github.io/" class="author-link" target="_blank">Wenhao Chai</a><sup> 1,2</sup> &emsp;
                    <a href="https://espere-1119-song.github.io/" class="author-link" target="_blank">Enxin Song</a> &emsp;
                    <a href="https://yilundu.github.io/" class="author-link" target="_blank">Yilun Du</a><sup> 4,5</sup> &emsp;
                    <a href="https://cs.stanford.edu/~chenlin/" class="author-link" target="_blank">Chenlin Meng</a><sup> 2,3</sup> &emsp;
                    <a href="https://scholar.google.com/citations?user=WjF1dugAAAAJ" class="author-link" target="_blank">Vashisht Madhavan</a><sup> 2</sup> &emsp;
                    <a href="https://omerbt.github.io/" class="author-link" target="_blank">Omer Bar-Tal</a><sup> 2</sup> &emsp;
                    <a href="https://people.ece.uw.edu/hwang/" class="author-link" target="_blank">Jeng-Neng Hwang</a><sup> 1</sup> &emsp;
                    <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie</a><sup> 6</sup> &emsp;
                    <a href="https://nlp.stanford.edu/~manning/" class="author-link" target="_blank">Christopher D. Manning</a><sup> 3</sup> &emsp;
                    </p>
                <p>
                    <sup>1</sup> <a href="https://www.washington.edu/" class="affiliation-link" id="affiliation" target="_blank">University of Washington</a> &emsp;
                    <sup>2</sup> <a href="https://pika.art" class="affiliation-link" id="affiliation" target="_blank">Pika Lab</a> &emsp;
                    <sup>3</sup> <a href="https://www.stanford.edu/" class="affiliation-link" id="affiliation" target="_blank">Stanford University</a> &emsp;
                    <br>
                    <sup>4</sup> <a href="https://www.mit.edu/" class="affiliation-link" id="affiliation" target="_blank">Massachusetts Institute of Technology</a> &emsp;
                    <sup>5</sup> <a href="https://www.harvard.edu/" class="affiliation-link" id="affiliation" target="_blank">Harvard University</a> &emsp;
                    <br>
                    <sup>6</sup> <a href="https://www.nyu.edu/" class="affiliation-link" id="affiliation" target="_blank">New York University</a>
                </p>
            </div>
        </div>
        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#auroracap">AuroraCap</a></div>
                <div><a href="#vdc">VDC</a></div>
            </nav>
        </d-contents>
        
        <p>
            We propose AuroraCap, a simple video captioner based on multimodal large language model. 
            We follow the simplest architecture design without additional parameters for temporal modeling. 
            To address the overhead caused by lengthy video sequences, we implement the token merging strategy, reducing the number of visual tokens input. 
            We present VDC, a video detailed captioning benchmark with over one thousand carefully annotated structured captions. 
            In addition, we propose a new LLM-assisted metric VDCScore for bettering evaluation. 
            We adopt a divide-and-conquer strategy to transform the evaluation of long captions into multiple short question-answering pairs. 
        </p>
        <section id="auroracap">
            <h2>AuroraCap: A Video Detailed Captioning Baseline</h2>
            <p>
                <h1 class="text">Architecture</h1>
                <p>
                    <strong>LLaVA.</strong> 
                    To effectively leverage the capabilities of both the pre-trained LLM and visual model, LLaVA adapt a simple multilayer perceptron (MLP) projection layer to connect each patch tokens of image features into the word embedding space. 
                </p>
                <p class="text">
                    <strong>Token merging.</strong> 
                    To increase the throughput of existing ViT models, Token Merging is proposed to gradually combines similar tokens in a transformer to reduce the number of tokens passing through ViT models. 
                    Token Merging has been proven to be effective on image and video classification tasks even without the need for training.
                    We conduct frame-wise token merging in AuroraCap, where the feature is extracted by CLIP ViT-H model. 
                    We show token merging visualization examples from  COCO, VG, SA-1B as follows:
                    <d-figure id="fig-cvcb" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/tome_visualize.png" alt="benchmark category">
                            <figcaption>
                                <strong>Figure 1:</strong> Token merging visualization. From top to bottom, the image IDs are COCO:COCO-train2014-000000247906, VG:2331508, SA-1B:sa-393200. From left to right, the number of tokens representing the images are 490, 154, 18, and 6.
                            </figcaption>
                        </figure>
                    </d-figure>
                </p>
            </p>
            <p>
                <h1 class="text">Training Recipe</h1>
                <p class="text">
                    <strong>Pretraining stage.</strong> 
                    Similar to LLaVA, we first align visual features with the word embedding space of LLMs. 
                    To achieve this, we freeze the pretrained ViT and LLM, training solely the vision-language connector. 
                </p>
                <p class="text">
                    <strong>Vision stage.</strong> 
                    Unlike LLaVA, we next unfreeze the pretrained ViT while freezing the LLM during vision stage and train with the public data among various computer vision tasks to get better generalization.
                </p>
                <p class="text">
                    <strong>Language stage.</strong> 
                    Finally, we conduct end-to-end training, which means all the components are trainable, with the most high-quality public data during language stage. 
                </p>
            </p>
            <p>
                <h1 class="text">Evaluation</h1>
                <p class="text">
                    <strong>Image Captioning.</strong> 
                    We evaluate AuroraCap using CIDEr, BELU-4, BELU-1, METEOR, and ROUGE-L metric on Flickr, NoCaps, and COCO-Cap benchmarks and compare it with LLM-based state-of-the-art methods. 
                    AuroraCap shows good performance under zero-shot settings. 
                    Notice that these benchmarks all contain short captions consisting of a single sentence, so they only partially reflect the model's performance.
                    <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                        <div class="table-container">
                            <table class="data-table">
                                <thead>
                                <tr>
                                    <th class="tb-hdr" rowspan="2">Model</th>
                                    <th colspan="2" class="tb-hdr">Flickr (31,784)</th>
                                    <th colspan="2" class="tb-hdr">NoCaps (4,500)</th>
                                    <th colspan="2" class="tb-hdr">COCO-Cap (5,000)</th>
                                </tr>
                                <tr>
                                    <th>C</th>
                                    <th>R</th>
                                    <th>C</th>
                                    <th>R</th>
                                    <th>C</th>
                                    <th>R</th>
                                </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td class="italic">LLaVA-1.5-7B</td>
                                        <td>74.9</td>
                                        <td>52.8</td>
                                        <td>105.5</td>
                                        <td>59.4</td>
                                        <td>110.3</td>
                                        <td>55.5</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLaVA-1.5-13B</td>
                                        <td>79.4</td>
                                        <td>53.9</td>
                                        <td>109.2</td>
                                        <td>60.3</td>
                                        <td>115.6</td>
                                        <td>56.5</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLaVA-1.6-7B</td>
                                        <td>68.4</td>
                                        <td>50.3</td>
                                        <td>88.4</td>
                                        <td>54.6</td>
                                        <td>99.9</td>
                                        <td>52.4</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">LLaVA-1.6-13B</td>
                                        <td>66.6</td>
                                        <td>48.8</td>
                                        <td>88.1</td>
                                        <td>54.9</td>
                                        <td>101.8</td>
                                        <td>52.1</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">MiniCPM-V-3B</td>
                                        <td>66.8</td>
                                        <td>51.0</td>
                                        <td>89.9</td>
                                        <td>55.8</td>
                                        <td>94.2</td>
                                        <td>52.3</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">DeCap</td>
                                        <td>56.7</td>
                                        <td>—</td>
                                        <td>42.7</td>
                                        <td>—</td>
                                        <td>91.2</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Flamingo-80B</td>
                                        <td>67.2</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>84.3</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Chameleon-34B</td>
                                        <td>74.7<sup>2</sup></td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>120.2<sup>2</sup></td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">GPT-4V</td>
                                        <td>55.3<sup>8</sup></td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>78.5<sup>8</sup></td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td class="italic">Gemini-1.5 Pro</td>
                                        <td>82.2<sup>4</sup></td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>99.8<sup>2</sup></td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td><strong>AuroraCap-7B</strong></td>
                                        <td class="highlight-red"><strong>88.9</strong></td>
                                        <td class="highlight-red"><strong>55.4</strong></td>
                                        <td class="highlight-red"><strong>111.4</strong></td>
                                        <td class="highlight-red"><strong>60.6</strong></td>
                                        <td class="highlight-red"><strong>120.8</strong></td>
                                        <td class="highlight-red"><strong>57.2</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <figcaption style="text-align: center; width: 140%;">
                            Table 2: Comparison AuroraCap with SoTA methods on video question answering and classification benchmarks under zero-shot setting.
                        </figcaption>
                    </div>
                </p>
                <p class="text">
                    <strong>Video Captioning.</strong> 
                    Although the current video captioning benchmarks are only contains one-sentence captions, to compare with prior work, we similarly evaluate on these benchmarks. 
                    We evaluate AuroraCap on MSR-VTT, VATEX, and ActivityNet Captions and compare it with other methods.
                    <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                        <div class="table-container">
                            <table class="data-table">
                                <thead>
                                <tr>
                                    <th class="tb-hdr" rowspan="2">Model</th>
                                    <th colspan="5" class="tb-hdr">MSR-VTT (1,000)</th>
                                    <th colspan="5" class="tb-hdr">VATEX (1,000)</th>
                                </tr>
                                <tr>
                                    <th>C</th>
                                    <th>B@1</th>
                                    <th>B@4</th>
                                    <th>M</th>
                                    <th>R</th>
                                    <th>C</th>
                                    <th>B@1</th>
                                    <th>B@4</th>
                                    <th>M</th>
                                    <th>R</th>
                                </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>ZeroCap</td>
                                        <td>9.6</td>
                                        <td>—</td>
                                        <td>2.9</td>
                                        <td>16.3</td>
                                        <td>35.4</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td>DeCap</td>
                                        <td>18.6</td>
                                        <td>—</td>
                                        <td>14.7</td>
                                        <td>20.4</td>
                                        <td>—</td>
                                        <td>18.7</td>
                                        <td>—</td>
                                        <td>13.1</td>
                                        <td>15.3</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td>PaLI-3</td>
                                        <td>21.3</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td>Ma et al.</td>
                                        <td>22.1</td>
                                        <td>—</td>
                                        <td>3.5</td>
                                        <td>17.3</td>
                                        <td>28.7</td>
                                        <td>23.9</td>
                                        <td>—</td>
                                        <td>2.8</td>
                                        <td>14.1</td>
                                        <td>23.5</td>
                                    </tr>
                                    <tr>
                                        <td>LLaVA-7B</td>
                                        <td>16.9</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                        <td>—</td>
                                    </tr>
                                    <tr>
                                        <td>Video-LLAMA</td>
                                        <td>2.3</td>
                                        <td>—</td>
                                        <td>4.9</td>
                                        <td>16.8</td>
                                        <td>—</td>
                                        <td>3.8</td>
                                        <td>—</td>
                                        <td>4.3</td>
                                        <td>16.3</td>
                                        <td>21.8</td>
                                    </tr>
                                    <tr>
                                        <td>AuroraCap-7B</td>
                                        <td class="highlight-red">33.1</td>
                                        <td class="highlight-red">58.6</td>
                                        <td class="highlight-red">21.0</td>
                                        <td class="highlight-red">23.9</td>
                                        <td class="highlight-red">49.5</td>
                                        <td class="highlight-red">33.8</td>
                                        <td class="highlight-red">57.1</td>
                                        <td class="highlight-red">18.4</td>
                                        <td class="highlight-red">19.0</td>
                                        <td class="highlight-red">40.8</td>
                                    </tr>

                                </tbody>
                            </table>
                        </div>
                        <figcaption style="text-align: center; width: 140%;">
                            Table 3: Comparison AuroraCap with SoTA methods on existing video captioning benchmarks under zero-shot setting.
                        </figcaption>
                    </div>
                </p>
            </p>
            <!-- <p>
                Multimodal LLMs (MLLMs) <d-cite key="blip2"></d-cite><d-cite key="flamingo"></d-cite><d-cite key="minigpt4"></d-cite><d-cite key="llava"></d-cite><d-cite key="InstructBLIP"></d-cite>  try to emulate humans' ability to integrate multimodal information and perform general tasks. Significant advances have been made in this domain, leveraging the strong reasoning capabilities of large language models.
                However, a key limitation of current MLLMs is their dependence on pre-trained (and often frozen) vision encoders, such as the CLIP <d-cite key="CLIP"></d-cite> image encoder. This dependency forms a major bottleneck for visual information processing. 
                The vision encoder is often trained on images with low resolution, such as 224&times;224 or 336&times;336 pixels. During deployment, images are also often resized to a lower resolution. As a result, the encoder may overlook important details in high-resolution images. Additionally, current MLLMs struggle to identify which essential visual details are missing or unclear in the images they process, nor can they proactively seek out or request this missing information. In such senarios, even the most powerfull multimodal intelligent system GPT-4V would fail while our MLLM with visual search mechanism can make correct responses.
                <d-figure>
                    <figure class="l-body">
                        <div id='figure1_div'></div>
                        <script src="figure1.js"></script>
                        <figcaption>
                            Examples on which GPT-4V fails while SEAL with the <i>V</i><sup>*</sup> visual search mechanism succeeds. Even though GPT-4V has a much more powerful LLM (GPT-4) than ours (Vicuna-7B), it still occasionally struggles in scenarios that demand extensive visual processing. These situations require precise visual grounding in high-resolution images, a task where the visual search mechanism becomes essential. Click the small images at the bottom to choose different examples.
                        </figcaption>
                    </figure>
                </d-figure>
            </p> -->
        </section>
        <section id="vdc">
            <h2>VDC: A Video Detailed Captioning Benchmark</h2>
            <p class="text">
                <h1 class="text">Benchmark Dataset Curation</h1>
                <strong>Video collection and processing.</strong>
                    We building VDC upon Panda-70M, Ego4D, Mixkit, Pixabay, and Pexels. 
                    Note that the videos used in VDC construction are not included in the training data of AuroraCap.
                    To ensure balanced data distribution, we allocate equal proportions of different video sources.  
                    We first split the video into clips and apply dense frame extraction, then manually replacing blurry frames with adjacent clear ones.
                <strong>Structured detailed captions construction pipeline.</strong>
                    We develop a structured detailed captions construction pipeline to generate extra detailed descriptions from various perspectives, significantly extending the length and enhancing the richness compared to previous benchmarks. 
                    The structured detailed captions includes the following categories:
                    <ol class="text">
                        <li><strong>Camera caption.</strong>
                            Describe the camera work in detail, including shot types, angles, movements, transitions, and any special effects used to enhance the video.
                        </li>
                        <li><strong>Short caption.</strong>
                            Summarize the video in one detailed sentence, capturing key actions and the overall mood.
                        </li>
                        <li><strong>Background caption.</strong>
                            Provide a detailed description of the background, including objects, location, weather, time, and any dynamic elements.
                        </li>
                        <li><strong>Main Object caption.</strong>
                            Give a thorough description of the main subject’s actions, attributes, interactions, and movements throughout the video frames.
                        </li>
                        <li><strong>Detailed caption.</strong>
                            Generate a detailed, vivid caption for the video, covering all categories, ensuring it’s engaging, informative, and rich enough for AI to recreate the video content.
                        </li>
                    </ol>
                    To generate detailed, fine-grained, and accurate captions, we leverage <b>GPT-4o</b> to produce video descriptions. 
                    We design a hierarchical prompt strategy to efficiently obtain accurate structured captions and detailed captions in two conversation rounds: (1) Structured Captions Generation and (2) Detailed Captions Integration. 
                <br>
                <strong>Comparison on numerical statistics.</strong>
                    The visual representation in Figure 2 demonstrates the video duration distribution of VDC and the length distribution of structured captions in VDC. 
                    <d-figure id="fig-vision_connector">
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/vdc.png" alt="Video length in VDC" style="width: 100%;display: block; margin: 0 auto;">
                            <figcaption>
                                <strong>Figure 2:</strong> Video length in VDC and distribution of structured caption length.
                            </figcaption>
                        </figure>
                    </d-figure>

            </p>
            <p class="text">
                <h1 class="text">Evaluation Metric Design and Leaderboard</h1>
                <strong>VDCScore: Evaluating Detailed Captions with LLMs</strong>
                    We building VDC upon Panda-70M, Ego4D, Mixkit, Pixabay, and Pexels. 
                    Note that the videos used in VDC construction are not included in the training data of AuroraCap.
                    To ensure balanced data distribution, we allocate equal proportions of different video sources.  
                    We first split the video into clips and apply dense frame extraction, then manually replacing blurry frames with adjacent clear ones.
                <strong>Structured detailed captions construction pipeline.</strong>
                    We develop a structured detailed captions construction pipeline to generate extra detailed descriptions from various perspectives, significantly extending the length and enhancing the richness compared to previous benchmarks. 
                    The structured detailed captions includes the following categories:
                    <ol class="text">
                        <li><strong>Camera caption.</strong>
                            Describe the camera work in detail, including shot types, angles, movements, transitions, and any special effects used to enhance the video.
                        </li>
                        <li><strong>Short caption.</strong>
                            Summarize the video in one detailed sentence, capturing key actions and the overall mood.
                        </li>
                        <li><strong>Background caption.</strong>
                            Provide a detailed description of the background, including objects, location, weather, time, and any dynamic elements.
                        </li>
                        <li><strong>Main Object caption.</strong>
                            Give a thorough description of the main subject’s actions, attributes, interactions, and movements throughout the video frames.
                        </li>
                        <li><strong>Detailed caption.</strong>
                            Generate a detailed, vivid caption for the video, covering all categories, ensuring it’s engaging, informative, and rich enough for AI to recreate the video content.
                        </li>
                    </ol>
                    To generate detailed, fine-grained, and accurate captions, we leverage <b>GPT-4o</b> to produce video descriptions. 
                    We design a hierarchical prompt strategy to efficiently obtain accurate structured captions and detailed captions in two conversation rounds: (1) Structured Captions Generation and (2) Detailed Captions Integration. 
                <br>
                <strong>Comparison on numerical statistics.</strong>
                    The visual representation in Figure 2 demonstrates the video duration distribution of VDC and the length distribution of structured captions in VDC. 
                    <d-figure id="fig-vision_connector">
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/vdc.png" alt="Video length in VDC" style="width: 100%;display: block; margin: 0 auto;">
                            <figcaption>
                                <strong>Figure 2:</strong> Video length in VDC and distribution of structured caption length.
                            </figcaption>
                        </figure>
                    </d-figure>

            </p>

            <h3 id="VQALLM">VQA LLM with Visual Working Memory</h3>
            <p>
                The visual search mechanism is not always engaged. The VQA LLM  first evaluates if the encoder's initial (global) visual features suffice for answering the question. If not, it explicitly lists all the needed but missing information in the format of a list of target objects. Then, it initializes a visual working memory (VWM). The VWM has four blocks, the &ltquestion&gt block contains the initial textual question; &ltglobal image&gt contains the initial image; &ltsearched targets&gt stores the target object crops after search; and &lttarget location&gt stores the coordinates of the searched targets. Next, the visual search model searches over the image and localizes each required target. A region containing the identified target is then cropped from the whole image. The cropped targets, along with their coordinates, are added to the VWM. After that, the VQA LLM processes the data contained in the VWM to generate the response accordingly. 
            </p>
            <h3 id="VisualSearch"><i>V</i><sup>*</sup>: LLM-guided Visual Search</h3>
            <p>
                Similar to how people often zoom in on their phones for a clearer view, when dealing with a high-resolution image, it's possible that the target object cannot be precisely identified and located if only the entire image is viewed as a small thumbnail.To address this, one straightforward approach is to patchify an image into uniformly sized small patches and perform the localization on each patch exhaustively. This brute-force strategy tends to be too inefficient for effectively managing images with very high resolutions -- we need a smarter solution.
            </p>
            <p>
                Drawing inspiration from how humans utilize contextual scene and top-down feature guidance in their visual search process, we've incorporated similar concepts into the design of the visual search model in <i>V</i><sup>*</sup>. This process utilizes an MLLM that encapsulates a vast amount of common sense knowledge, serving as heuristic guidance. In order to localize and crop the searched targets for VWM, it's also necessary to enhance the MLLM with additional localization capabilities.
            </p>
            <p>With this visual search model, our <i>V</i><sup>*</sup> algorithm works as follows. Given an image and a textual expression of the target object, the <i>V</i><sup>*</sup> MLLM first attempts to locate the target directly. In this step, we obtain the target localization results (coordinates and confidence) and the search cue heatmap. When no object is located (the confidence score falls below a threshold), we scrutinize the search cue heatmap for possible target-specific cues. 
    
                The search cue heatmap highlights regions that could potentially contain the queried target object. When the target-specific cue is prominent (<i>ie.</i> when the highest value in the heatmap exceeds the threshold \(\delta\)), we use it to guide the search directly. Otherwise, we ask the MLLM what is the most likely location of the target object in the image. This requires the MLLM to utilize its common sense knowledge and integrate it with the image's context to provide the contextual cue about the target's whereabouts. Upon receiving a description of the region where the target object is likely located, we prompt the MLLM to locate the described area and produce a search cue heatmap corresponding to the contextual cue.
                
                Then, we use a simple strategy and recursively divide the image into 4 non-overlapping equal-sized patches. Subsequently, we assign search priority scores to these patches. The search priority score is calculated from the search cue heatmap (either target-specific or contextual). Based on the priority scores, the patches are then cropped and processed sequentially. This recursive procedure is repeated until the target object is located or the size of the current patch becomes smaller than a predetermined threshold.
    
                <d-figure>
                    <figure>
                        <img src="images/vs_algo.jpg" alt="LLM-guided Visual Search Algorithm">
                        <!-- <figcaption>An instantiation of the proposed SEAL framework. The left part is the VQA LLM which takes all the information in the VWM to answer questions. The right part demonstrates the LLM-guided visual search process.</figcaption> -->
                    </figure>
                </d-figure>
            
            </p>
            <h3 id="Astar">Connection to <i>A</i><sup>*</sup> Algorithm</h3>
            <p>
                The naming of our LLM-guided visual search <i>V</i><sup>*</sup> algorithm is inspired by its similarities to the informed search algorithm <i>A</i><sup>*</sup>. <i>A</i><sup>*</sup> is designed for pathfinding, aiming to identify the shortest route between a starting point and a goal by using a heuristic to approximate the cost. In the context of our LLM-guided visual search, <i>V</i><sup>*</sup> can be seen as a unique variant of <i>A</i><sup>*</sup>, where sub-images are treated as nodes. The cost function \(g(n)\) is set as a uniform positive constant for all \(n\) and the heuristic function \(h(n)\) is defined as the negative of the priority score derived from the search cue heatmap.  While the <i>A</i><sup>*</sup> algorithm's objective is to find a path with minimal cost from start to goal, our focus with <i>V</i><sup>*</sup> is solely on minimizing the total number of steps required to locate the goal.
            </p>
        </section>
        
        <section id="Vstarbench">
            <h2><i>V</i><sup>*</sup>Bench</h2>  
            <p>
                To quantitatively evaluate MLLMs' ability in challenging scenarios where the image contains abundant and complex information and the visual information needed might not be easily found, we build a benchmark <i>V</i><sup>*</sup>Bench based on 191 high-resolution images from SAM <d-cite key="SAM"></d-cite> with an average image resolution of 2246 <span>&#215;</span> 1582.
    
                <i>V</i><sup>*</sup>Bench contains two sub-tasks: attribute recognition and spatial relationship reasoning. The attribute recognition task has 115 samples and requires the model to recognize a certain type of attribute (<i>eg.</i> color, material) of an object. The spatial relationship reasoning task has 76 samples and asks the model to determine the relative spatial relationship between two objects. These tasks focus on evaluating the detailed visual analysis capability of the multimodal models.
    
                <d-figure>
                    <figure class="l-body">
                        <div id='figure2_div'></div>
                        <script src="figure2.js"></script>
                        <figcaption>
                            Examples of <i>V</i><sup>*</sup>Bench. The top row belongs to the attribute recognition task while the bottom row belongs to the spatial relationship reasoning task. The correct option is in green. Click the small images at the bottom to choose different examples.
                        </figcaption>
                    </figure>
                </d-figure>
                We evaluate SEAL with other open-source end-to-end MLLMs and LLM-tool-using systems on the proposed <i>V</i><sup>*</sup>.
    
                <d-figure>
                        <img src="images/table.jpg" alt="<i>V</i><sup>*</sup> Results" width="60%" style="display: block; margin-left: auto; margin-right: auto;">
                        <figcaption>Evaluation of multimodal systems on <i>V</i><sup>*</sup>Bench. Our model shows superior performance, validating the necessity of incorporating the visual search mechanism into MLLMs.</figcaption>
                </d-figure> 
                <gradio-app src="https://craigwu-vstar.hf.space"></gradio-app>
            </p>
        </section>
        
        <section id="VisualSearchExp">
            <h2>Effectiveness of <i>V</i><sup>*</sup></h2>
            <p>
                We evaluate different search strategies in terms of search length on 245 target objects in <i>V</i><sup>*</sup>Bench. The search length here is defined as the number of search steps from the initial image to the patch where the target is located. We compare our LLM-guided search strategy with two baselines. The random baseline adopts the random strategy to pick a random sub-image to explore, and the sequential baseline searches the sub-images in sequential order. These two strategies are evaluated in breadth-first search (BFS) and depth-first search settings respectively.
                <d-figure>
                    <img src="images/search_path.jpg" alt="<i>V</i><sup>*</sup> Results" width="50%" style="display: block; margin-left: auto; margin-right: auto;">
                    <figcaption>Evaluation of different search strategies in terms of search length on <i>V</i><sup>*</sup>Bench. Our LLM-guided search could greatly reduce the average search length and both two guiding cues are helpful for the search process</figcaption>
                </d-figure>

                <d-figure>
                    <figure class="l-body">
                        <div id='figure3_div'></div>
                        <script src="figure3.js"></script>
                        <figcaption>
                            Examples of the LLM-guided visual search process. Click the arrow to see different step of the visual search process. Click the small images at the bottom to choose different examples.
                        </figcaption>
                    </figure>
                </d-figure>


                To further study the efficiency of <i>V</i><sup>*</sup> algorithm and draw parallels with cognitive science research in visual search, we conduct comparisons between our search outcomes and human behaviors using the COCO-Search18 dataset <d-cite key="cocosearch18"></d-cite>. COCO-Search18 records people's eye fixations when searching for a specific target object in natural scene images. We convert the ground-truth human fixation sequence on each sample to a 2D heatmap and use it as guidance during the search. Interestingly, <i>V</i><sup>*</sup> algorithm can achieve similar efficiency to the human fixations.

                <d-figure>
                    <img src="images/cocosearch_result.jpg" alt="Coco search Results" width="50%" style="display: block; margin-left: auto; margin-right: auto;">
                    <figcaption>Compare our LLM-guided <i>V</i><sup>*</sup> visual search algorithm with the human fixation and other search strategies on COCO-Search18.</figcaption>
                </d-figure>

                <d-figure>
                    <img src="images/cocosearch_vis.jpg" alt="Coco search Visualization" width="60%" style="display: block; margin-left: auto; margin-right: auto;">
                    <figcaption> Visualization of search cues of our <i>V</i><sup>*</sup> algorithm and human fixation on COCO-Search18. Humans tend to focus on center regions or salient objects while our model focuses on a larger contextual region.</figcaption>
                </d-figure>
                
            </p>
        </section>
        
        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{vstar,<br>
                &nbsp;&nbsp;title={V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs},<br>
                &nbsp;&nbsp;author={Penghao Wu and Saining Xie},<br>
                &nbsp;&nbsp;year={2023},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2312.14135},<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
          </d-appendix>
          
            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>

          <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>

        <!-- <script type="text/bibliography">
        
        </script> -->
        <script src="contents_bar.js"></script>
        
        

    </body>
</html>